{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PHOC Model Implementation**\n",
    "\n",
    "This notebook is an implementation of the PHOCNet architecture for our Cropped Word Recognition dataset and task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directory Manager\n",
    "- To manage the different required file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the generated trained model\n",
    "saved_model_phocnet = 'PATH'\n",
    "\n",
    "# Lexicon file (file containing necessary words -> knn, data generation...)\n",
    "lexicon_file = 'PATH' \n",
    "\n",
    "# Bigrams for phoc\n",
    "bigrams_file = 'PATH' \n",
    "\n",
    "# Access the trained KNN Classifier\n",
    "store_knn_classifier = 'PATH'\n",
    "\n",
    "# Image CSV files (image_path/label) in this case our generated dataset\n",
    "train_images_csv = 'PATH'\n",
    "test_images_csv = 'PATH'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.optim.lr_scheduler import StepLR, CyclicLR, CosineAnnealingLR, ReduceLROnPlateau\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "\n",
    "!pip install editdistance\n",
    "import editdistance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model + SPP\n",
    "\n",
    "- PHOCNet pipeline\n",
    "- SPP to use any input size images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial pyramid pooling implementation\n",
    "class SPP(nn.Module):\n",
    "\n",
    "    def __init__(self, levels = 3, pool_type = 'max_pool'):\n",
    "        super(SPP, self).__init__()\n",
    "\n",
    "        if pool_type not in ['max_pool', 'avg_pool', 'max_avg_pool']:\n",
    "            raise ValueError('Unknown pool_type. Must be either \\'max_pool\\', \\'avg_pool\\' or both')\n",
    "        \n",
    "        self.pooling_output_size = sum([4 ** level for level in range(levels)]) * 512\n",
    "\n",
    "        self.levels = levels\n",
    "        self.pool_type = pool_type\n",
    "\n",
    "    def forward(self, input_x):\n",
    "        out = self._spatial_pyramid_pooling(input_x, self.levels)\n",
    "        return out\n",
    "    \n",
    "    def _pyramid_pooling(self, input_x, output_sizes):\n",
    "        pyramid_level_tensors = []\n",
    "        for tsize in output_sizes:\n",
    "            if self.pool_type == 'max_pool':\n",
    "                pyramid_level_tensor = F.adaptive_max_pool2d(input_x, tsize)\n",
    "                pyramid_level_tensor = pyramid_level_tensor.view(input_x.size(0), -1)\n",
    "            if self.pool_type == 'avg_pool':\n",
    "                pyramid_level_tensor = F.adaptive_avg_pool2d(input_x, tsize)\n",
    "                pyramid_level_tensor = pyramid_level_tensor.view(input_x.size(0), -1)\n",
    "            if self.pool_type == 'max_avg_pool':\n",
    "                pyramid_level_tensor_max = F.adaptive_max_pool2d(input_x, tsize)\n",
    "                pyramid_level_tensor_max = pyramid_level_tensor_max.view(input_x.size(0), -1)\n",
    "                pyramid_level_tensor_avg = F.adaptive_avg_pool2d(input_x, tsize)\n",
    "                pyramid_level_tensor_avg = pyramid_level_tensor_avg.view(input_x.size(0), -1)\n",
    "                pyramid_level_tensor = torch.cat([pyramid_level_tensor_max, pyramid_level_tensor_avg], dim=1)\n",
    "\n",
    "            pyramid_level_tensors.append(pyramid_level_tensor)\n",
    "\n",
    "        return torch.cat(pyramid_level_tensors, dim=1)\n",
    "\n",
    "    def _spatial_pyramid_pooling(self, input_x, levels):\n",
    "        output_sizes = [(int( 2 **level), int( 2 **level)) for level in range(levels)]\n",
    "        return self._pyramid_pooling(input_x, output_sizes)\n",
    "\n",
    "\n",
    "# PHOCnet architecture\n",
    "class PHOCNet(nn.Module):\n",
    "\n",
    "    def __init__(self, n_out, input_channels = 3, pooling_levels = 3, pool_type = 'max_pool'):\n",
    "        super(PHOCNet, self).__init__()\n",
    "\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_channels, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv_block4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.pooling_layer_fn = SPP(levels = pooling_levels, pool_type=pool_type)\n",
    "        pooling_output_size = self.pooling_layer_fn.pooling_output_size\n",
    "        \n",
    "        self.fc1 = nn.Linear(pooling_output_size, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.Linear(4096, n_out)\n",
    "\n",
    "    \n",
    "    # Forward function (organized by blocks as the implementation)\n",
    "    def forward(self, x):\n",
    "        out = self.conv_block1(x)\n",
    "        out = F.max_pool2d(out, kernel_size=2, stride=2, padding=0)\n",
    "        out = self.conv_block2(out)\n",
    "        out = F.max_pool2d(out, kernel_size=2, stride=2, padding=0)\n",
    "        out = self.conv_block3(out)\n",
    "        out = self.conv_block4(out)\n",
    "\n",
    "        out = self.pooling_layer_fn(out)\n",
    "        out = self.fc1(out)\n",
    "        out = F.relu(out)\n",
    "        out = F.dropout(out, p = 0.5, training = self.training)\n",
    "        out = self.fc2(out)\n",
    "        out = F.relu(out)\n",
    "        out = F.dropout(out, p = 0.5, training = self.training)\n",
    "        out = self.fc3(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PHOC Vector functions\n",
    "\n",
    "- Functions to obtain PHOC vector of any string of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_phoc(words, phoc_unigrams, unigram_levels,\n",
    "               bigram_levels=None, phoc_bigrams=None,\n",
    "               split_character=None):\n",
    "    '''\n",
    "    Calculate Pyramidal Histogram of Characters (PHOC) descriptor\n",
    "    \n",
    "        word (str): word to calculate descriptor for\n",
    "        phoc_unigrams (str): string of all unigrams to use in the PHOC\n",
    "        unigram_levels (list of int): the levels for the unigrams in PHOC\n",
    "        phoc_bigrams (list of str): list of bigrams to be used in the PHOC\n",
    "        phoc_bigram_levls (list of int): the levels of the bigrams in the PHOC\n",
    "        split_character (str): special character to split the word strings into characters\n",
    "        on_unknown_unigram (str): What to do if a unigram appearing in a word\n",
    "            is not among the supplied phoc_unigrams. Possible: 'warn', 'error'\n",
    "\n",
    "    Returns\n",
    "        the PHOC for the given word\n",
    "    '''\n",
    "    # prepare output matrix\n",
    "    phoc_size = len(phoc_unigrams) * np.sum(unigram_levels)\n",
    "    if phoc_bigrams is not None:\n",
    "        phoc_size += len(phoc_bigrams) * np.sum(bigram_levels)\n",
    "    phocs = np.zeros((len(words), phoc_size))\n",
    "\n",
    "    # prepare some lambda functions\n",
    "    occupancy = lambda k, n: [float(k) / n, float(k + 1) / n]\n",
    "    overlap = lambda a, b: [max(a[0], b[0]), min(a[1], b[1])]\n",
    "    size = lambda region: region[1] - region[0]\n",
    "\n",
    "    # map from character to alphabet position\n",
    "    char_indices = {d: i for i, d in enumerate(phoc_unigrams)}\n",
    "\n",
    "    # iterate through all the words\n",
    "    for word_index, word in enumerate(words):\n",
    "        \"\"\"if '0' in word or '1' in word or '2' in word or '3' in word or '4' in word or '5' in word or '6' in word or '7' in word or '7' in word or '8' in word or '9' in word:\n",
    "            continue\"\"\"\n",
    "        if split_character is not None:\n",
    "            word = word.split(split_character)\n",
    "        n = len(word)\n",
    "        for index, char in enumerate(word):\n",
    "            char_occ = occupancy(index, n)        \n",
    "            char_index = char_indices[char]\n",
    "\n",
    "            for level in unigram_levels:\n",
    "                for region in range(level):\n",
    "                    region_occ = occupancy(region, level)\n",
    "                    if size(overlap(char_occ, region_occ)) / size(char_occ) >= 0.5:\n",
    "                        feat_vec_index = sum([l for l in unigram_levels if l < level]) * len(\n",
    "                            phoc_unigrams) + region * len(phoc_unigrams) + char_index\n",
    "                        phocs[word_index, feat_vec_index] = 1\n",
    "\n",
    "        # add bigrams\n",
    "        if phoc_bigrams is not None:\n",
    "            ngram_features = np.zeros(len(phoc_bigrams) * np.sum(bigram_levels))\n",
    "            ngram_occupancy = lambda k, n: [float(k) / n, float(k + 2) / n]\n",
    "\n",
    "            for i in range(n - 1):\n",
    "                ngram = word[i:i + 2]\n",
    "                phoc_dict = {k: v for v, k in enumerate(phoc_bigrams)}\n",
    "                if phoc_dict.get(ngram, 666) == 666:\n",
    "                    continue\n",
    "                occ = ngram_occupancy(i, n)\n",
    "\n",
    "                for level in bigram_levels:\n",
    "                    for region in range(level):\n",
    "                        region_occ = occupancy(region, level)\n",
    "                        overlap_size = size(overlap(occ, region_occ)) / size(occ)\n",
    "                        if overlap_size >= 0.5:\n",
    "                            ngram_features[region * len(phoc_bigrams) + phoc_dict[ngram]] = 1\n",
    "            phocs[word_index, -ngram_features.shape[0]:] = ngram_features\n",
    "\n",
    "    return phocs\n",
    "\n",
    "def phoc(raw_word):\n",
    "    '''\n",
    "\n",
    "    :param raw_word: string of word to be converted\n",
    "    :return: phoc representation as a np.array (1,604)\n",
    "    '''\n",
    "    if type(raw_word) == type([]):\n",
    "        word = [w.lower() for w in raw_word]\n",
    "    else:\n",
    "        word =[raw_word]\n",
    "        word_lowercase = word[0].lower()\n",
    "        word = [word_lowercase]\n",
    "    phoc_unigrams = '0123456789abcdefghijklmnopqrstuvwxyz'\n",
    "    unigram_levels = [2,3,4,5]\n",
    "    bigram_levels=[]\n",
    "    bigram_levels.append(2)\n",
    "    \n",
    "    phoc_bigrams = []\n",
    "    i = 0\n",
    "    with open(bigrams_file,'r') as f:\n",
    "        for line in f:\n",
    "            a = line.split()\n",
    "            phoc_bigrams.append(a[0].lower())\n",
    "            i = i +1\n",
    "            if i >= 50:break\n",
    "\n",
    "    \n",
    "    qry_phocs = build_phoc(words = word, phoc_unigrams = phoc_unigrams, unigram_levels = unigram_levels,\n",
    "                           bigram_levels = bigram_levels, phoc_bigrams = phoc_bigrams)\n",
    "    qry_phocs = build_phoc(words = word, phoc_unigrams = phoc_unigrams, unigram_levels = unigram_levels)\n",
    "    \n",
    "    return qry_phocs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Generator\n",
    "\n",
    "- To generate a KNN as it effectively classifies character histograms by the local neighborhood information, enhancing text recognition accuracy\n",
    "- No need to run again if the KNN Classifier has already been created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'with open(lexicon_file, \"r\") as file:\\n    list_of_words = file.readlines()\\n\\n\\n\"\"\"img_dir = \"\"\\nlist_of_words = os.listdir(img_dir)\\nlist_of_words = [word.split(\"_\")[-1].split(\".\")[0] for word in list_of_words]\"\"\"\\nlist_of_words = [l[:-1] for l in list_of_words]\\nphoc_representations = phoc(list_of_words)\\n\\nknn = KNeighborsClassifier(n_neighbors=1)\\nknn.fit(phoc_representations, list_of_words)\\n\\nknnPickle = open(store_knn_classifier, \\'wb\\') \\npickle.dump(knn, knnPickle)  \\nknnPickle.close()'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(lexicon_file, \"r\") as file:\n",
    "    list_of_words = file.readlines()\n",
    "\n",
    "list_of_words = [l[:-1] for l in list_of_words]\n",
    "phoc_representations = phoc(list_of_words)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(phoc_representations, list_of_words)\n",
    "\n",
    "knnPickle = open(store_knn_classifier, 'wb') \n",
    "pickle.dump(knn, knnPickle)  \n",
    "knnPickle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset \n",
    "\n",
    "- Pass images, their PHOC-Vector label and the label in string format\n",
    "- Image transforms as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, img_dir, transform = None):\n",
    "        \n",
    "        self.paths = os.listdir(img_dir)\n",
    "        #self.paths = self.paths[:int(len(self.paths)*0.01)]\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        path = self.img_dir + self.paths[idx]\n",
    "\n",
    "        img = read_image(path)\n",
    "        img = img.to(torch.float32)\n",
    "        if self.transform != None:\n",
    "            img = self.transform(img)\n",
    "        img /= 255\n",
    "        \n",
    "        word = self.paths[idx].split(\"_\")[-1].split(\".\")[0]\n",
    "        target = phoc(word)\n",
    "        target = target.reshape(target.shape[1])\n",
    "\n",
    "        return img, target, word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions\n",
    "\n",
    "- load_model() -> load the KNN model\n",
    "\n",
    "- predict_with_PHOC() -> predict a label using the PHOC model\n",
    "- get_data() -> create a custom pytorch 'dataset' from an image path \n",
    "- make_loader() -> loads the created 'dataset' using a pytorch dataloader\n",
    "- make() -> initialize the model with all of its attritbutes, data, optimizer, scheduler criterion & weights \n",
    "- init_weights_model() -> initiates the models for the weights using a kaiming normal distribution\n",
    "- create_weights() -> creates a tensor for the weights in the training section: the idea is to define the weights for an error as the inverse of the 'appeareance frequency' of the that word in the lexicon.txt file\n",
    "- set_parameter_requires_grad() -> freezes the parameters of a neural network model, preventing their gradients from being computed during training, when the feature_extracting flag is set to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    # Load the pre-trained KNN model from a file\n",
    "    return pickle.load(open(store_knn_classifier, 'rb'))\n",
    "\n",
    "def predict_with_PHOC(phocs, model):\n",
    "    # Predict labels using the KNN model with PHOC representations\n",
    "    result = model.predict(phocs)\n",
    "    return result\n",
    "\n",
    "def get_data(img_dir, transform=None):\n",
    "    # Create a custom dataset from the image directory with optional transformations\n",
    "    created_dataset = dataset(img_dir, transform)\n",
    "    return created_dataset\n",
    "\n",
    "def make_loader(dataset, batch_size):\n",
    "    # Create a DataLoader for the dataset with the specified batch size and other parameters\n",
    "    loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                         batch_size=batch_size, \n",
    "                                         shuffle=True,\n",
    "                                         pin_memory=True, num_workers=2)\n",
    "    return loader\n",
    "\n",
    "def make(config, device=\"cuda\"):\n",
    "    # Initialize the model, data loaders, optimizer, scheduler, and loss function\n",
    "    print(\"Starting the make function...\")\n",
    "\n",
    "    transforms_train = transforms.Compose([\n",
    "        transforms.Resize((64, 128), antialias=True),\n",
    "        transforms.Normalize(mean=[0.445313568], std=[0.26924618])\n",
    "    ])\n",
    "    print(\"Training transforms created.\")\n",
    "\n",
    "    transforms_test = transforms.Compose([\n",
    "        transforms.Resize((64, 128), antialias=True),\n",
    "        transforms.Normalize(mean=[0.445313568], std=[0.26924618])\n",
    "    ])\n",
    "    print(\"Testing transforms created.\")\n",
    "\n",
    "    train, test = get_data(config.train_dir, transforms_train), get_data(config.test_dir, transforms_test)\n",
    "    print(f\"Training data and testing data loaded. Number of training samples: {len(train)}, Number of testing samples: {len(test)}\")\n",
    "\n",
    "    train_loader = make_loader(train, config.batch_size)\n",
    "    test_loader = make_loader(test, config.batch_size)\n",
    "    print(f\"Data loaders created. Batch size: {config.batch_size}\")\n",
    "\n",
    "    # Make the model\n",
    "    model = PHOCNet(n_out=train[0][1].shape[0], input_channels=1).to(device)\n",
    "    model.apply(init_weights_model)\n",
    "    print(\"Model created and weights initialized.\")\n",
    "\n",
    "    pos_weight = torch.tensor(create_weights(config.train_dir)).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction='mean', pos_weight=pos_weight)\n",
    "    print(f\"Loss function created with pos_weight: {pos_weight}\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "    print(f\"Optimizer created with learning rate: {config.learning_rate}\")\n",
    "\n",
    "    scheduler = ReduceLROnPlateau(optimizer, patience=2, factor=0.1)\n",
    "    print(\"Scheduler created.\")\n",
    "\n",
    "    print(\"Make function completed.\")\n",
    "    return model, train_loader, test_loader, criterion, optimizer, scheduler\n",
    "\n",
    "def init_weights_model(m):\n",
    "    # Initialize weights of Conv2d and Linear layers with Kaiming normal distribution\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "def create_weights(file_words):\n",
    "    # Create weights for the loss function based on the frequency of words in the dataset\n",
    "    paths = os.listdir(file_words)\n",
    "    list_of_words = [path.split(\"_\")[-1].split(\".\")[0] for path in paths]\n",
    "    list_of_words = [l[:-1] for l in list_of_words]\n",
    "    phoc_representations = phoc(list_of_words)\n",
    "    suma = np.sum(phoc_representations, axis=0)\n",
    "    weights = phoc_representations.shape[0]/(suma+1e-6) \n",
    "    weights = (1 + (weights/max(weights)))*5\n",
    "    return weights\n",
    "\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    # Freeze parameters of the model if feature_extracting is True\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wandb functions\n",
    "\n",
    "- To keep track of our execution data on wandb and draw images with their corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (0.17.0)\n",
      "Requirement already satisfied: typing-extensions; python_version < \"3.10\" in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from wandb) (4.5.0)\n",
      "Requirement already satisfied: setproctitle in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from wandb) (2.3.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from wandb) (2.28.2)\n",
      "Requirement already satisfied: platformdirs in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from wandb) (3.2.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from wandb) (8.0.4)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from wandb) (3.1.31)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from wandb) (5.9.4)\n",
      "Requirement already satisfied: setuptools in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from wandb) (49.6.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0; python_version < \"3.9\" and sys_platform == \"linux\" in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from wandb) (3.19.6)\n",
      "Requirement already satisfied: pyyaml in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: urllib3>=1.26.11 in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from sentry-sdk>=1.0.0->wandb) (1.26.15)\n",
      "Requirement already satisfied: certifi in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from sentry-sdk>=1.0.0->wandb) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (3.1.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: six>=1.4.0 in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n"
     ]
    }
   ],
   "source": [
    "# Install the wandb library for experiment tracking and logging\n",
    "!pip install wandb\n",
    "\n",
    "import wandb\n",
    "from torchvision import transforms as T\n",
    "from PIL import ImageDraw, ImageFont  # Using PIL to draw labels on the images\n",
    "from torchvision import transforms\n",
    "\n",
    "def train_log(loss, total_example_ct):\n",
    "    # Log training loss to wandb and print the current loss\n",
    "    wandb.log({\"loss\": loss}, step=total_example_ct)\n",
    "    print(f\"Loss after {str(total_example_ct).zfill(5)} examples: {loss:.3f}\")\n",
    "\n",
    "def train_test_log(loss_test, loss_train, accuracy_test, accuracy_train, edit_test, edit_train, epoch):\n",
    "    # Log training and testing metrics to wandb and print the losses\n",
    "    wandb.log({\n",
    "        \"Epoch\": epoch, \n",
    "        \"Train loss\": loss_train, \"Test loss\": loss_test,\n",
    "        \"Train accuracy\": accuracy_train, \"Test accuracy\": accuracy_test,\n",
    "        \"Train edit\": edit_train, \"Test edit\": edit_test,\n",
    "    })\n",
    "    print(f\"Train Loss: {loss_train:.3f}\\nTest Loss: {loss_test:.3f}\")\n",
    "\n",
    "def log_images(images, predicted_labels, text_labels, epoch, mode):\n",
    "    # Log images with predicted and true labels to wandb\n",
    "    t = transforms.Compose([transforms.Normalize(0, 1/0.1), transforms.Normalize(-0.5, 1)])\n",
    "    t_images = t(images)\n",
    "    images_with_labels = draw_images(t_images, text_labels, predicted_labels)\n",
    "    wandb.log({f\"Epoch{epoch}-{mode}\": [wandb.Image(im) for im in images_with_labels]})\n",
    "\n",
    "def draw_images(images, text_labels, predicted_labels):\n",
    "    # Convert images to PIL format and draw labels on them\n",
    "    transform = T.ToPILImage()\n",
    "    images = [draw_one_image(transform(im), t_lab, p_lab) for im, t_lab, p_lab in zip(images, text_labels, predicted_labels)]\n",
    "    return images\n",
    "\n",
    "def draw_one_image(image, text_label, predicted_label):\n",
    "    # Draw text labels on a single image and color code based on correctness\n",
    "    image = image.convert(\"RGB\")\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    color = \"green\" if text_label == predicted_label else \"red\"\n",
    "    text = text_label + \"\\n\" + predicted_label\n",
    "    font = ImageFont.truetype(f'/home/xnmaster/DlProject/deep-learning-project-2024-ai_nndl_group_01_/Generate Dataset/Fonts/calibri.ttf', 10)\n",
    "    draw.text((0, 0), text, font=font, fill=color)\n",
    "    return image\n",
    "\n",
    "def lr_log(lr, epoch):\n",
    "    # Log the learning rate to wandb\n",
    "    wandb.log({\"learning-rate\": lr}, step=epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, test_loader, criterion, optimizer, scheduler, config, device = \"cuda\"):\n",
    "    # Tell wandb to watch what the model gets up to: gradients, weights...\n",
    "    wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
    "\n",
    "    example_ct = 0  # number of examples seen\n",
    "    batch_ct = 0 \n",
    "    model_phoc = load_model() # Loading the KNN model\n",
    "    for epoch in (range(config.epochs)):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for _, (images, phoc_labels, _) in enumerate(train_loader):\n",
    "\n",
    "            loss = train_batch(images, phoc_labels, model, optimizer, criterion, device)\n",
    "            train_loss += loss.item()\n",
    "            example_ct +=  len(images)\n",
    "            batch_ct += 1\n",
    "\n",
    "            # Report metrics every 25 batches\n",
    "            if ((batch_ct + 1) % 25) == 0:\n",
    "                train_log(loss.item(), example_ct)\n",
    "        \n",
    "        loss_test = test(model, test_loader, train_loader, epoch, criterion, model_phoc, device)\n",
    "        \n",
    "        scheduler.step(loss_test)\n",
    "        print(scheduler._last_lr)\n",
    "        torch.save(model.state_dict(), os.path.join(config.save_model, f\"PHOCNET{epoch}.pt\"))\n",
    "    return model\n",
    "\n",
    "\n",
    "# Train on individual batch\n",
    "def train_batch(images, labels, model, optimizer, criterion, device=\"cuda\"): # GPU if possible\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs.float(), labels.float())\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Optim step\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: editdistance in /anaconda/envs/azureml_py38_PT_and_TF/lib/python3.8/site-packages (0.8.1)\n"
     ]
    }
   ],
   "source": [
    "def test(model, test_loader, train_loader, epoch, criterion, model_phoc, device=\"cuda\", save: bool = True):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Initialize variables to track losses, correct predictions, and edit distances\n",
    "        loss_test = 0\n",
    "        loss_train = 0\n",
    "        correct_test = 0\n",
    "        correct_train = 0\n",
    "        edit_test = 0\n",
    "        edit_train = 0\n",
    "        test_count = 0\n",
    "        train_count = 0\n",
    "        \n",
    "        # Iterate over the test data\n",
    "        for i, (images, phoc_labels, text_labels) in enumerate(test_loader):\n",
    "            images, phoc_labels = images.to(device), phoc_labels.to(device)\n",
    "            test_count += len(images)\n",
    "            outputs = model(images)\n",
    "            loss_test += criterion(outputs, phoc_labels.float())\n",
    "            predicted_labels = predict_with_PHOC(torch.sigmoid(outputs).cpu().numpy(), model_phoc)\n",
    "            correct_test += (predicted_labels == text_labels).sum().item()\n",
    "            edit_test += sum([editdistance.eval(p, t) for p, t in zip(predicted_labels, text_labels)])\n",
    "            if i == 0:\n",
    "                log_images(images, predicted_labels, text_labels[:5], epoch, \"Test\")\n",
    "                \n",
    "        # Iterate over the training data\n",
    "        for i, (images, phoc_labels, text_labels) in enumerate(train_loader):\n",
    "            images, phoc_labels = images.to(device), phoc_labels.to(device)\n",
    "            train_count += len(images)\n",
    "            outputs = model(images)\n",
    "            loss_train += criterion(outputs, phoc_labels.float())\n",
    "            predicted_labels = predict_with_PHOC(torch.sigmoid(outputs).cpu().numpy(), model_phoc)\n",
    "            correct_train += (predicted_labels == text_labels).sum().item()\n",
    "            edit_train += sum([editdistance.eval(p, t) for p, t in zip(predicted_labels, text_labels)])\n",
    "            if i == 0:\n",
    "                log_images(images, predicted_labels, text_labels[:5], epoch, \"Train\")\n",
    "            if i == 150:\n",
    "                break\n",
    "\n",
    "        # Calculate average losses, accuracies, and edit distances\n",
    "        loss_test = loss_test / len(test_loader)\n",
    "        loss_train = loss_train / (i + 1)\n",
    "        accuracy_test = correct_test / test_count\n",
    "        accuracy_train = correct_train / train_count\n",
    "        edit_test = edit_test / test_count\n",
    "        edit_train = edit_train / train_count\n",
    "\n",
    "        # Log training and testing metrics\n",
    "        train_test_log(loss_test, loss_train, accuracy_test, accuracy_train, edit_test, edit_train, epoch)\n",
    "    \n",
    "    # Return the test loss\n",
    "    return loss_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/xnmaster/DlProject/deep-learning-project-2024-ai_nndl_group_01_/PHOC/Generated Data Trial/wandb/run-20240530_003443-2y7o6nyc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marino-com/phoc-def/runs/2y7o6nyc' target=\"_blank\">lemon-cloud-10</a></strong> to <a href='https://wandb.ai/marino-com/phoc-def' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marino-com/phoc-def' target=\"_blank\">https://wandb.ai/marino-com/phoc-def</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marino-com/phoc-def/runs/2y7o6nyc' target=\"_blank\">https://wandb.ai/marino-com/phoc-def/runs/2y7o6nyc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the make function...\n",
      "Training transforms created.\n",
      "Testing transforms created.\n",
      "Training data and testing data loaded. Number of training samples: 50000, Number of testing samples: 5000\n",
      "Data loaders created. Batch size: 8\n",
      "Model created and weights initialized.\n",
      "Loss function created with pos_weight: tensor([ 5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "        10.0000,  5.0000, 10.0000, 10.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000, 10.0000,  5.0000, 10.0000, 10.0000, 10.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "        10.0000,  5.0000,  5.0000, 10.0000, 10.0000, 10.0000,  5.0000, 10.0000,\n",
      "         5.0000, 10.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000, 10.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "        10.0000,  5.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,\n",
      "         5.0000, 10.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000, 10.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000, 10.0000, 10.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,\n",
      "         5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000,  5.0000],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "Optimizer created with learning rate: 0.0001\n",
      "Scheduler created.\n",
      "Make function completed.\n",
      "Loss after 00192 examples: 0.677\n",
      "Loss after 00392 examples: 0.583\n",
      "Loss after 00592 examples: 0.567\n",
      "Loss after 00792 examples: 0.603\n",
      "Loss after 00992 examples: 0.529\n",
      "Loss after 01192 examples: 0.534\n",
      "Loss after 01392 examples: 0.545\n",
      "Loss after 01592 examples: 0.531\n",
      "Loss after 01792 examples: 0.529\n",
      "Loss after 01992 examples: 0.518\n",
      "Loss after 02192 examples: 0.518\n",
      "Loss after 02392 examples: 0.519\n",
      "Loss after 02592 examples: 0.575\n",
      "Loss after 02792 examples: 0.560\n",
      "Loss after 02992 examples: 0.516\n",
      "Loss after 03192 examples: 0.533\n",
      "Loss after 03392 examples: 0.507\n",
      "Loss after 03592 examples: 0.521\n",
      "Loss after 03792 examples: 0.508\n",
      "Loss after 03992 examples: 0.484\n",
      "Loss after 04192 examples: 0.501\n",
      "Loss after 04392 examples: 0.533\n",
      "Loss after 04592 examples: 0.554\n",
      "Loss after 04792 examples: 0.518\n",
      "Loss after 04992 examples: 0.537\n",
      "Loss after 05192 examples: 0.505\n",
      "Loss after 05392 examples: 0.528\n",
      "Loss after 05592 examples: 0.517\n",
      "Loss after 05792 examples: 0.508\n",
      "Loss after 05992 examples: 0.502\n",
      "Loss after 06192 examples: 0.457\n",
      "Loss after 06392 examples: 0.578\n",
      "Loss after 06592 examples: 0.565\n",
      "Loss after 06792 examples: 0.573\n",
      "Loss after 06992 examples: 0.485\n",
      "Loss after 07192 examples: 0.536\n",
      "Loss after 07392 examples: 0.541\n",
      "Loss after 07592 examples: 0.508\n",
      "Loss after 07792 examples: 0.506\n",
      "Loss after 07992 examples: 0.499\n",
      "Loss after 08192 examples: 0.508\n",
      "Loss after 08392 examples: 0.515\n",
      "Loss after 08592 examples: 0.512\n",
      "Loss after 08792 examples: 0.535\n",
      "Loss after 08992 examples: 0.500\n",
      "Loss after 09192 examples: 0.544\n",
      "Loss after 09392 examples: 0.481\n",
      "Loss after 09592 examples: 0.460\n",
      "Loss after 09792 examples: 0.502\n",
      "Loss after 09992 examples: 0.462\n",
      "Loss after 10192 examples: 0.522\n",
      "Loss after 10392 examples: 0.488\n",
      "Loss after 10592 examples: 0.483\n",
      "Loss after 10792 examples: 0.490\n",
      "Loss after 10992 examples: 0.525\n",
      "Loss after 11192 examples: 0.519\n",
      "Loss after 11392 examples: 0.475\n",
      "Loss after 11592 examples: 0.437\n",
      "Loss after 11792 examples: 0.493\n",
      "Loss after 11992 examples: 0.478\n",
      "Loss after 12192 examples: 0.493\n",
      "Loss after 12392 examples: 0.451\n",
      "Loss after 12592 examples: 0.451\n",
      "Loss after 12792 examples: 0.499\n",
      "Loss after 12992 examples: 0.474\n",
      "Loss after 13192 examples: 0.510\n",
      "Loss after 13392 examples: 0.473\n",
      "Loss after 13592 examples: 0.485\n",
      "Loss after 13792 examples: 0.509\n",
      "Loss after 13992 examples: 0.430\n",
      "Loss after 14192 examples: 0.476\n",
      "Loss after 14392 examples: 0.502\n",
      "Loss after 14592 examples: 0.503\n",
      "Loss after 14792 examples: 0.420\n",
      "Loss after 14992 examples: 0.489\n",
      "Loss after 15192 examples: 0.475\n",
      "Loss after 15392 examples: 0.502\n",
      "Loss after 15592 examples: 0.440\n",
      "Loss after 15792 examples: 0.455\n",
      "Loss after 15992 examples: 0.497\n",
      "Loss after 16192 examples: 0.443\n",
      "Loss after 16392 examples: 0.457\n",
      "Loss after 16592 examples: 0.502\n",
      "Loss after 16792 examples: 0.448\n",
      "Loss after 16992 examples: 0.435\n",
      "Loss after 17192 examples: 0.426\n",
      "Loss after 17392 examples: 0.358\n",
      "Loss after 17592 examples: 0.480\n",
      "Loss after 17792 examples: 0.425\n",
      "Loss after 17992 examples: 0.503\n",
      "Loss after 18192 examples: 0.415\n",
      "Loss after 18392 examples: 0.362\n",
      "Loss after 18592 examples: 0.418\n",
      "Loss after 18792 examples: 0.431\n",
      "Loss after 18992 examples: 0.411\n",
      "Loss after 19192 examples: 0.498\n",
      "Loss after 19392 examples: 0.420\n",
      "Loss after 19592 examples: 0.334\n",
      "Loss after 19792 examples: 0.360\n",
      "Loss after 19992 examples: 0.368\n",
      "Loss after 20192 examples: 0.438\n",
      "Loss after 20392 examples: 0.392\n",
      "Loss after 20592 examples: 0.485\n",
      "Loss after 20792 examples: 0.358\n",
      "Loss after 20992 examples: 0.419\n",
      "Loss after 21192 examples: 0.420\n",
      "Loss after 21392 examples: 0.446\n",
      "Loss after 21592 examples: 0.428\n",
      "Loss after 21792 examples: 0.406\n",
      "Loss after 21992 examples: 0.366\n",
      "Loss after 22192 examples: 0.351\n",
      "Loss after 22392 examples: 0.385\n",
      "Loss after 22592 examples: 0.374\n",
      "Loss after 22792 examples: 0.356\n",
      "Loss after 22992 examples: 0.406\n",
      "Loss after 23192 examples: 0.434\n",
      "Loss after 23392 examples: 0.325\n",
      "Loss after 23592 examples: 0.367\n",
      "Loss after 23792 examples: 0.354\n",
      "Loss after 23992 examples: 0.388\n",
      "Loss after 24192 examples: 0.321\n",
      "Loss after 24392 examples: 0.254\n",
      "Loss after 24592 examples: 0.401\n",
      "Loss after 24792 examples: 0.329\n",
      "Loss after 24992 examples: 0.342\n",
      "Loss after 25192 examples: 0.281\n",
      "Loss after 25392 examples: 0.329\n",
      "Loss after 25592 examples: 0.349\n",
      "Loss after 25792 examples: 0.251\n",
      "Loss after 25992 examples: 0.320\n",
      "Loss after 26192 examples: 0.318\n",
      "Loss after 26392 examples: 0.331\n",
      "Loss after 26592 examples: 0.294\n",
      "Loss after 26792 examples: 0.338\n",
      "Loss after 26992 examples: 0.304\n",
      "Loss after 27192 examples: 0.247\n",
      "Loss after 27392 examples: 0.304\n",
      "Loss after 27592 examples: 0.322\n",
      "Loss after 27792 examples: 0.260\n",
      "Loss after 27992 examples: 0.323\n",
      "Loss after 28192 examples: 0.332\n",
      "Loss after 28392 examples: 0.306\n",
      "Loss after 28592 examples: 0.214\n",
      "Loss after 28792 examples: 0.251\n",
      "Loss after 28992 examples: 0.264\n",
      "Loss after 29192 examples: 0.243\n",
      "Loss after 29392 examples: 0.233\n",
      "Loss after 29592 examples: 0.241\n",
      "Loss after 29792 examples: 0.236\n",
      "Loss after 29992 examples: 0.222\n",
      "Loss after 30192 examples: 0.248\n",
      "Loss after 30392 examples: 0.276\n",
      "Loss after 30592 examples: 0.232\n",
      "Loss after 30792 examples: 0.228\n",
      "Loss after 30992 examples: 0.244\n",
      "Loss after 31192 examples: 0.235\n",
      "Loss after 31392 examples: 0.219\n",
      "Loss after 31592 examples: 0.201\n",
      "Loss after 31792 examples: 0.246\n",
      "Loss after 31992 examples: 0.178\n",
      "Loss after 32192 examples: 0.224\n",
      "Loss after 32392 examples: 0.223\n",
      "Loss after 32592 examples: 0.172\n",
      "Loss after 32792 examples: 0.155\n",
      "Loss after 32992 examples: 0.164\n",
      "Loss after 33192 examples: 0.169\n",
      "Loss after 33392 examples: 0.195\n",
      "Loss after 33592 examples: 0.117\n",
      "Loss after 33792 examples: 0.200\n",
      "Loss after 33992 examples: 0.251\n",
      "Loss after 34192 examples: 0.171\n",
      "Loss after 34392 examples: 0.189\n",
      "Loss after 34592 examples: 0.163\n",
      "Loss after 34792 examples: 0.146\n",
      "Loss after 34992 examples: 0.191\n",
      "Loss after 35192 examples: 0.137\n",
      "Loss after 35392 examples: 0.173\n",
      "Loss after 35592 examples: 0.157\n",
      "Loss after 35792 examples: 0.122\n",
      "Loss after 35992 examples: 0.135\n",
      "Loss after 36192 examples: 0.139\n",
      "Loss after 36392 examples: 0.159\n",
      "Loss after 36592 examples: 0.170\n",
      "Loss after 36792 examples: 0.188\n",
      "Loss after 36992 examples: 0.118\n",
      "Loss after 37192 examples: 0.181\n",
      "Loss after 37392 examples: 0.154\n",
      "Loss after 37592 examples: 0.114\n",
      "Loss after 37792 examples: 0.128\n",
      "Loss after 37992 examples: 0.168\n",
      "Loss after 38192 examples: 0.186\n",
      "Loss after 38392 examples: 0.208\n",
      "Loss after 38592 examples: 0.120\n",
      "Loss after 38792 examples: 0.133\n",
      "Loss after 38992 examples: 0.171\n",
      "Loss after 39192 examples: 0.122\n",
      "Loss after 39392 examples: 0.153\n",
      "Loss after 39592 examples: 0.141\n",
      "Loss after 39792 examples: 0.128\n",
      "Loss after 39992 examples: 0.145\n",
      "Loss after 40192 examples: 0.166\n",
      "Loss after 40392 examples: 0.105\n",
      "Loss after 40592 examples: 0.130\n",
      "Loss after 40792 examples: 0.148\n",
      "Loss after 40992 examples: 0.185\n",
      "Loss after 41192 examples: 0.107\n",
      "Loss after 41392 examples: 0.142\n",
      "Loss after 41592 examples: 0.101\n",
      "Loss after 41792 examples: 0.169\n",
      "Loss after 41992 examples: 0.125\n",
      "Loss after 42192 examples: 0.107\n",
      "Loss after 42392 examples: 0.132\n",
      "Loss after 42592 examples: 0.182\n",
      "Loss after 42792 examples: 0.148\n",
      "Loss after 42992 examples: 0.128\n",
      "Loss after 43192 examples: 0.122\n",
      "Loss after 43392 examples: 0.119\n",
      "Loss after 43592 examples: 0.109\n",
      "Loss after 43792 examples: 0.136\n",
      "Loss after 43992 examples: 0.145\n",
      "Loss after 44192 examples: 0.143\n",
      "Loss after 44392 examples: 0.107\n",
      "Loss after 44592 examples: 0.141\n",
      "Loss after 44792 examples: 0.146\n",
      "Loss after 44992 examples: 0.134\n",
      "Loss after 45192 examples: 0.091\n",
      "Loss after 45392 examples: 0.124\n",
      "Loss after 45592 examples: 0.086\n",
      "Loss after 45792 examples: 0.112\n",
      "Loss after 45992 examples: 0.149\n",
      "Loss after 46192 examples: 0.095\n",
      "Loss after 46392 examples: 0.131\n",
      "Loss after 46592 examples: 0.127\n",
      "Loss after 46792 examples: 0.081\n",
      "Loss after 46992 examples: 0.159\n",
      "Loss after 47192 examples: 0.081\n",
      "Loss after 47392 examples: 0.099\n",
      "Loss after 47592 examples: 0.080\n",
      "Loss after 47792 examples: 0.105\n",
      "Loss after 47992 examples: 0.116\n",
      "Loss after 48192 examples: 0.134\n",
      "Loss after 48392 examples: 0.080\n",
      "Loss after 48592 examples: 0.081\n",
      "Loss after 48792 examples: 0.123\n",
      "Loss after 48992 examples: 0.065\n",
      "Loss after 49192 examples: 0.181\n",
      "Loss after 49392 examples: 0.084\n",
      "Loss after 49592 examples: 0.138\n",
      "Loss after 49792 examples: 0.119\n",
      "Loss after 49992 examples: 0.096\n",
      "Train Loss: 0.076\n",
      "Test Loss: 0.076\n",
      "[0.0001]\n",
      "Loss after 50192 examples: 0.100\n",
      "Loss after 50392 examples: 0.088\n",
      "Loss after 50592 examples: 0.100\n",
      "Loss after 50792 examples: 0.095\n",
      "Loss after 50992 examples: 0.082\n",
      "Loss after 51192 examples: 0.137\n",
      "Loss after 51392 examples: 0.112\n",
      "Loss after 51592 examples: 0.090\n",
      "Loss after 51792 examples: 0.077\n",
      "Loss after 51992 examples: 0.106\n",
      "Loss after 52192 examples: 0.094\n",
      "Loss after 52392 examples: 0.118\n",
      "Loss after 52592 examples: 0.095\n",
      "Loss after 52792 examples: 0.081\n",
      "Loss after 52992 examples: 0.105\n",
      "Loss after 53192 examples: 0.090\n",
      "Loss after 53392 examples: 0.106\n",
      "Loss after 53592 examples: 0.074\n",
      "Loss after 53792 examples: 0.085\n",
      "Loss after 53992 examples: 0.069\n",
      "Loss after 54192 examples: 0.101\n",
      "Loss after 54392 examples: 0.079\n",
      "Loss after 54592 examples: 0.099\n",
      "Loss after 54792 examples: 0.086\n",
      "Loss after 54992 examples: 0.127\n",
      "Loss after 55192 examples: 0.140\n",
      "Loss after 55392 examples: 0.104\n",
      "Loss after 55592 examples: 0.124\n",
      "Loss after 55792 examples: 0.098\n",
      "Loss after 55992 examples: 0.101\n",
      "Loss after 56192 examples: 0.098\n",
      "Loss after 56392 examples: 0.082\n",
      "Loss after 56592 examples: 0.100\n",
      "Loss after 56792 examples: 0.105\n",
      "Loss after 56992 examples: 0.110\n",
      "Loss after 57192 examples: 0.111\n",
      "Loss after 57392 examples: 0.072\n",
      "Loss after 57592 examples: 0.066\n",
      "Loss after 57792 examples: 0.101\n",
      "Loss after 57992 examples: 0.085\n",
      "Loss after 58192 examples: 0.068\n",
      "Loss after 58392 examples: 0.087\n",
      "Loss after 58592 examples: 0.086\n",
      "Loss after 58792 examples: 0.079\n",
      "Loss after 58992 examples: 0.076\n",
      "Loss after 59192 examples: 0.089\n",
      "Loss after 59392 examples: 0.086\n",
      "Loss after 59592 examples: 0.118\n",
      "Loss after 59792 examples: 0.078\n",
      "Loss after 59992 examples: 0.093\n",
      "Loss after 60192 examples: 0.078\n",
      "Loss after 60392 examples: 0.106\n",
      "Loss after 60592 examples: 0.091\n",
      "Loss after 60792 examples: 0.106\n",
      "Loss after 60992 examples: 0.074\n",
      "Loss after 61192 examples: 0.104\n",
      "Loss after 61392 examples: 0.064\n",
      "Loss after 61592 examples: 0.106\n",
      "Loss after 61792 examples: 0.069\n",
      "Loss after 61992 examples: 0.085\n",
      "Loss after 62192 examples: 0.065\n",
      "Loss after 62392 examples: 0.081\n",
      "Loss after 62592 examples: 0.078\n",
      "Loss after 62792 examples: 0.073\n",
      "Loss after 62992 examples: 0.090\n",
      "Loss after 63192 examples: 0.091\n",
      "Loss after 63392 examples: 0.103\n",
      "Loss after 63592 examples: 0.095\n",
      "Loss after 63792 examples: 0.092\n",
      "Loss after 63992 examples: 0.076\n",
      "Loss after 64192 examples: 0.078\n",
      "Loss after 64392 examples: 0.092\n",
      "Loss after 64592 examples: 0.064\n",
      "Loss after 64792 examples: 0.077\n",
      "Loss after 64992 examples: 0.080\n",
      "Loss after 65192 examples: 0.073\n",
      "Loss after 65392 examples: 0.065\n",
      "Loss after 65592 examples: 0.063\n",
      "Loss after 65792 examples: 0.074\n",
      "Loss after 65992 examples: 0.107\n",
      "Loss after 66192 examples: 0.072\n",
      "Loss after 66392 examples: 0.093\n",
      "Loss after 66592 examples: 0.079\n",
      "Loss after 66792 examples: 0.073\n",
      "Loss after 66992 examples: 0.107\n",
      "Loss after 67192 examples: 0.103\n",
      "Loss after 67392 examples: 0.112\n",
      "Loss after 67592 examples: 0.083\n",
      "Loss after 67792 examples: 0.087\n",
      "Loss after 67992 examples: 0.082\n",
      "Loss after 68192 examples: 0.088\n",
      "Loss after 68392 examples: 0.085\n",
      "Loss after 68592 examples: 0.083\n",
      "Loss after 68792 examples: 0.080\n",
      "Loss after 68992 examples: 0.084\n",
      "Loss after 69192 examples: 0.072\n",
      "Loss after 69392 examples: 0.063\n",
      "Loss after 69592 examples: 0.080\n",
      "Loss after 69792 examples: 0.068\n",
      "Loss after 69992 examples: 0.120\n",
      "Loss after 70192 examples: 0.088\n",
      "Loss after 70392 examples: 0.098\n",
      "Loss after 70592 examples: 0.074\n",
      "Loss after 70792 examples: 0.071\n",
      "Loss after 70992 examples: 0.076\n",
      "Loss after 71192 examples: 0.083\n",
      "Loss after 71392 examples: 0.169\n",
      "Loss after 71592 examples: 0.070\n",
      "Loss after 71792 examples: 0.078\n",
      "Loss after 71992 examples: 0.069\n",
      "Loss after 72192 examples: 0.069\n",
      "Loss after 72392 examples: 0.111\n",
      "Loss after 72592 examples: 0.060\n",
      "Loss after 72792 examples: 0.079\n",
      "Loss after 72992 examples: 0.075\n",
      "Loss after 73192 examples: 0.096\n",
      "Loss after 73392 examples: 0.078\n",
      "Loss after 73592 examples: 0.094\n",
      "Loss after 73792 examples: 0.064\n",
      "Loss after 73992 examples: 0.100\n",
      "Loss after 74192 examples: 0.055\n",
      "Loss after 74392 examples: 0.118\n",
      "Loss after 74592 examples: 0.084\n",
      "Loss after 74792 examples: 0.057\n",
      "Loss after 74992 examples: 0.052\n",
      "Loss after 75192 examples: 0.065\n",
      "Loss after 75392 examples: 0.106\n",
      "Loss after 75592 examples: 0.081\n",
      "Loss after 75792 examples: 0.099\n",
      "Loss after 75992 examples: 0.081\n",
      "Loss after 76192 examples: 0.077\n",
      "Loss after 76392 examples: 0.095\n",
      "Loss after 76592 examples: 0.090\n",
      "Loss after 76792 examples: 0.077\n",
      "Loss after 76992 examples: 0.058\n",
      "Loss after 77192 examples: 0.098\n",
      "Loss after 77392 examples: 0.079\n",
      "Loss after 77592 examples: 0.077\n",
      "Loss after 77792 examples: 0.095\n",
      "Loss after 77992 examples: 0.094\n",
      "Loss after 78192 examples: 0.080\n",
      "Loss after 78392 examples: 0.097\n",
      "Loss after 78592 examples: 0.056\n",
      "Loss after 78792 examples: 0.065\n",
      "Loss after 78992 examples: 0.080\n",
      "Loss after 79192 examples: 0.058\n",
      "Loss after 79392 examples: 0.081\n",
      "Loss after 79592 examples: 0.093\n",
      "Loss after 79792 examples: 0.089\n",
      "Loss after 79992 examples: 0.071\n",
      "Loss after 80192 examples: 0.082\n",
      "Loss after 80392 examples: 0.092\n",
      "Loss after 80592 examples: 0.094\n",
      "Loss after 80792 examples: 0.075\n",
      "Loss after 80992 examples: 0.059\n",
      "Loss after 81192 examples: 0.058\n",
      "Loss after 81392 examples: 0.060\n",
      "Loss after 81592 examples: 0.064\n",
      "Loss after 81792 examples: 0.051\n",
      "Loss after 81992 examples: 0.073\n",
      "Loss after 82192 examples: 0.079\n",
      "Loss after 82392 examples: 0.063\n",
      "Loss after 82592 examples: 0.063\n",
      "Loss after 82792 examples: 0.073\n",
      "Loss after 82992 examples: 0.062\n",
      "Loss after 83192 examples: 0.082\n",
      "Loss after 83392 examples: 0.088\n",
      "Loss after 83592 examples: 0.088\n",
      "Loss after 83792 examples: 0.084\n",
      "Loss after 83992 examples: 0.074\n",
      "Loss after 84192 examples: 0.090\n",
      "Loss after 84392 examples: 0.100\n",
      "Loss after 84592 examples: 0.070\n",
      "Loss after 84792 examples: 0.057\n",
      "Loss after 84992 examples: 0.069\n",
      "Loss after 85192 examples: 0.066\n",
      "Loss after 85392 examples: 0.064\n",
      "Loss after 85592 examples: 0.042\n",
      "Loss after 85792 examples: 0.067\n",
      "Loss after 85992 examples: 0.066\n",
      "Loss after 86192 examples: 0.082\n",
      "Loss after 86392 examples: 0.076\n",
      "Loss after 86592 examples: 0.076\n",
      "Loss after 86792 examples: 0.056\n",
      "Loss after 86992 examples: 0.074\n",
      "Loss after 87192 examples: 0.102\n",
      "Loss after 87392 examples: 0.056\n",
      "Loss after 87592 examples: 0.063\n",
      "Loss after 87792 examples: 0.056\n",
      "Loss after 87992 examples: 0.074\n",
      "Loss after 88192 examples: 0.075\n",
      "Loss after 88392 examples: 0.072\n",
      "Loss after 88592 examples: 0.089\n",
      "Loss after 88792 examples: 0.060\n",
      "Loss after 88992 examples: 0.046\n",
      "Loss after 89192 examples: 0.072\n",
      "Loss after 89392 examples: 0.093\n",
      "Loss after 89592 examples: 0.073\n",
      "Loss after 89792 examples: 0.063\n",
      "Loss after 89992 examples: 0.069\n",
      "Loss after 90192 examples: 0.069\n",
      "Loss after 90392 examples: 0.050\n",
      "Loss after 90592 examples: 0.091\n",
      "Loss after 90792 examples: 0.053\n",
      "Loss after 90992 examples: 0.059\n",
      "Loss after 91192 examples: 0.087\n",
      "Loss after 91392 examples: 0.089\n",
      "Loss after 91592 examples: 0.050\n",
      "Loss after 91792 examples: 0.067\n",
      "Loss after 91992 examples: 0.044\n",
      "Loss after 92192 examples: 0.065\n",
      "Loss after 92392 examples: 0.064\n",
      "Loss after 92592 examples: 0.062\n",
      "Loss after 92792 examples: 0.074\n",
      "Loss after 92992 examples: 0.075\n",
      "Loss after 93192 examples: 0.055\n",
      "Loss after 93392 examples: 0.046\n",
      "Loss after 93592 examples: 0.065\n",
      "Loss after 93792 examples: 0.067\n",
      "Loss after 93992 examples: 0.068\n",
      "Loss after 94192 examples: 0.080\n",
      "Loss after 94392 examples: 0.072\n",
      "Loss after 94592 examples: 0.126\n",
      "Loss after 94792 examples: 0.089\n",
      "Loss after 94992 examples: 0.044\n",
      "Loss after 95192 examples: 0.072\n",
      "Loss after 95392 examples: 0.087\n",
      "Loss after 95592 examples: 0.071\n",
      "Loss after 95792 examples: 0.053\n",
      "Loss after 95992 examples: 0.054\n",
      "Loss after 96192 examples: 0.072\n",
      "Loss after 96392 examples: 0.069\n",
      "Loss after 96592 examples: 0.081\n",
      "Loss after 96792 examples: 0.065\n",
      "Loss after 96992 examples: 0.082\n",
      "Loss after 97192 examples: 0.082\n",
      "Loss after 97392 examples: 0.088\n",
      "Loss after 97592 examples: 0.059\n",
      "Loss after 97792 examples: 0.063\n",
      "Loss after 97992 examples: 0.052\n",
      "Loss after 98192 examples: 0.061\n",
      "Loss after 98392 examples: 0.076\n",
      "Loss after 98592 examples: 0.072\n",
      "Loss after 98792 examples: 0.073\n",
      "Loss after 98992 examples: 0.058\n",
      "Loss after 99192 examples: 0.071\n",
      "Loss after 99392 examples: 0.064\n",
      "Loss after 99592 examples: 0.076\n",
      "Loss after 99792 examples: 0.050\n",
      "Loss after 99992 examples: 0.069\n",
      "Train Loss: 0.042\n",
      "Test Loss: 0.044\n",
      "[0.0001]\n",
      "Loss after 100192 examples: 0.064\n",
      "Loss after 100392 examples: 0.079\n",
      "Loss after 100592 examples: 0.058\n",
      "Loss after 100792 examples: 0.046\n",
      "Loss after 100992 examples: 0.055\n",
      "Loss after 101192 examples: 0.097\n",
      "Loss after 101392 examples: 0.047\n",
      "Loss after 101592 examples: 0.045\n",
      "Loss after 101792 examples: 0.044\n",
      "Loss after 101992 examples: 0.047\n",
      "Loss after 102192 examples: 0.102\n",
      "Loss after 102392 examples: 0.054\n",
      "Loss after 102592 examples: 0.048\n",
      "Loss after 102792 examples: 0.056\n",
      "Loss after 102992 examples: 0.043\n",
      "Loss after 103192 examples: 0.071\n",
      "Loss after 103392 examples: 0.040\n",
      "Loss after 103592 examples: 0.062\n",
      "Loss after 103792 examples: 0.045\n",
      "Loss after 103992 examples: 0.048\n",
      "Loss after 104192 examples: 0.059\n",
      "Loss after 104392 examples: 0.032\n",
      "Loss after 104592 examples: 0.063\n",
      "Loss after 104792 examples: 0.055\n",
      "Loss after 104992 examples: 0.067\n",
      "Loss after 105192 examples: 0.049\n",
      "Loss after 105392 examples: 0.068\n",
      "Loss after 105592 examples: 0.052\n",
      "Loss after 105792 examples: 0.078\n",
      "Loss after 105992 examples: 0.046\n",
      "Loss after 106192 examples: 0.053\n",
      "Loss after 106392 examples: 0.066\n",
      "Loss after 106592 examples: 0.066\n",
      "Loss after 106792 examples: 0.069\n",
      "Loss after 106992 examples: 0.052\n",
      "Loss after 107192 examples: 0.057\n",
      "Loss after 107392 examples: 0.064\n",
      "Loss after 107592 examples: 0.054\n",
      "Loss after 107792 examples: 0.056\n",
      "Loss after 107992 examples: 0.043\n",
      "Loss after 108192 examples: 0.046\n",
      "Loss after 108392 examples: 0.046\n",
      "Loss after 108592 examples: 0.057\n",
      "Loss after 108792 examples: 0.050\n",
      "Loss after 108992 examples: 0.049\n",
      "Loss after 109192 examples: 0.042\n",
      "Loss after 109392 examples: 0.080\n",
      "Loss after 109592 examples: 0.054\n",
      "Loss after 109792 examples: 0.099\n",
      "Loss after 109992 examples: 0.044\n",
      "Loss after 110192 examples: 0.072\n",
      "Loss after 110392 examples: 0.072\n",
      "Loss after 110592 examples: 0.045\n",
      "Loss after 110792 examples: 0.062\n",
      "Loss after 110992 examples: 0.075\n",
      "Loss after 111192 examples: 0.048\n",
      "Loss after 111392 examples: 0.054\n",
      "Loss after 111592 examples: 0.062\n",
      "Loss after 111792 examples: 0.072\n",
      "Loss after 111992 examples: 0.041\n",
      "Loss after 112192 examples: 0.055\n",
      "Loss after 112392 examples: 0.063\n",
      "Loss after 112592 examples: 0.059\n",
      "Loss after 112792 examples: 0.039\n",
      "Loss after 112992 examples: 0.058\n",
      "Loss after 113192 examples: 0.055\n",
      "Loss after 113392 examples: 0.055\n",
      "Loss after 113592 examples: 0.040\n",
      "Loss after 113792 examples: 0.043\n",
      "Loss after 113992 examples: 0.062\n",
      "Loss after 114192 examples: 0.050\n",
      "Loss after 114392 examples: 0.050\n",
      "Loss after 114592 examples: 0.054\n",
      "Loss after 114792 examples: 0.062\n",
      "Loss after 114992 examples: 0.055\n",
      "Loss after 115192 examples: 0.067\n",
      "Loss after 115392 examples: 0.079\n",
      "Loss after 115592 examples: 0.053\n",
      "Loss after 115792 examples: 0.082\n",
      "Loss after 115992 examples: 0.063\n",
      "Loss after 116192 examples: 0.053\n",
      "Loss after 116392 examples: 0.066\n",
      "Loss after 116592 examples: 0.061\n",
      "Loss after 116792 examples: 0.048\n",
      "Loss after 116992 examples: 0.041\n",
      "Loss after 117192 examples: 0.061\n",
      "Loss after 117392 examples: 0.068\n",
      "Loss after 117592 examples: 0.075\n",
      "Loss after 117792 examples: 0.024\n",
      "Loss after 117992 examples: 0.049\n",
      "Loss after 118192 examples: 0.061\n",
      "Loss after 118392 examples: 0.043\n",
      "Loss after 118592 examples: 0.053\n",
      "Loss after 118792 examples: 0.046\n",
      "Loss after 118992 examples: 0.059\n",
      "Loss after 119192 examples: 0.033\n",
      "Loss after 119392 examples: 0.052\n",
      "Loss after 119592 examples: 0.048\n",
      "Loss after 119792 examples: 0.071\n",
      "Loss after 119992 examples: 0.063\n",
      "Loss after 120192 examples: 0.065\n",
      "Loss after 120392 examples: 0.057\n",
      "Loss after 120592 examples: 0.078\n",
      "Loss after 120792 examples: 0.084\n",
      "Loss after 120992 examples: 0.039\n",
      "Loss after 121192 examples: 0.064\n",
      "Loss after 121392 examples: 0.059\n",
      "Loss after 121592 examples: 0.061\n",
      "Loss after 121792 examples: 0.035\n",
      "Loss after 121992 examples: 0.047\n",
      "Loss after 122192 examples: 0.049\n",
      "Loss after 122392 examples: 0.068\n",
      "Loss after 122592 examples: 0.044\n",
      "Loss after 122792 examples: 0.044\n",
      "Loss after 122992 examples: 0.068\n",
      "Loss after 123192 examples: 0.042\n",
      "Loss after 123392 examples: 0.041\n",
      "Loss after 123592 examples: 0.048\n",
      "Loss after 123792 examples: 0.059\n",
      "Loss after 123992 examples: 0.063\n",
      "Loss after 124192 examples: 0.056\n",
      "Loss after 124392 examples: 0.056\n",
      "Loss after 124592 examples: 0.061\n",
      "Loss after 124792 examples: 0.044\n",
      "Loss after 124992 examples: 0.049\n",
      "Loss after 125192 examples: 0.070\n",
      "Loss after 125392 examples: 0.048\n",
      "Loss after 125592 examples: 0.032\n",
      "Loss after 125792 examples: 0.040\n",
      "Loss after 125992 examples: 0.043\n",
      "Loss after 126192 examples: 0.057\n",
      "Loss after 126392 examples: 0.062\n",
      "Loss after 126592 examples: 0.061\n",
      "Loss after 126792 examples: 0.037\n",
      "Loss after 126992 examples: 0.049\n",
      "Loss after 127192 examples: 0.037\n",
      "Loss after 127392 examples: 0.069\n",
      "Loss after 127592 examples: 0.052\n",
      "Loss after 127792 examples: 0.052\n",
      "Loss after 127992 examples: 0.040\n",
      "Loss after 128192 examples: 0.048\n",
      "Loss after 128392 examples: 0.078\n",
      "Loss after 128592 examples: 0.056\n",
      "Loss after 128792 examples: 0.056\n",
      "Loss after 128992 examples: 0.039\n",
      "Loss after 129192 examples: 0.058\n",
      "Loss after 129392 examples: 0.044\n",
      "Loss after 129592 examples: 0.080\n",
      "Loss after 129792 examples: 0.041\n",
      "Loss after 129992 examples: 0.060\n",
      "Loss after 130192 examples: 0.075\n",
      "Loss after 130392 examples: 0.052\n",
      "Loss after 130592 examples: 0.057\n",
      "Loss after 130792 examples: 0.050\n",
      "Loss after 130992 examples: 0.065\n",
      "Loss after 131192 examples: 0.051\n",
      "Loss after 131392 examples: 0.053\n",
      "Loss after 131592 examples: 0.054\n",
      "Loss after 131792 examples: 0.061\n",
      "Loss after 131992 examples: 0.047\n",
      "Loss after 132192 examples: 0.035\n",
      "Loss after 132392 examples: 0.067\n",
      "Loss after 132592 examples: 0.094\n",
      "Loss after 132792 examples: 0.071\n",
      "Loss after 132992 examples: 0.054\n",
      "Loss after 133192 examples: 0.060\n",
      "Loss after 133392 examples: 0.039\n",
      "Loss after 133592 examples: 0.061\n",
      "Loss after 133792 examples: 0.041\n",
      "Loss after 133992 examples: 0.061\n",
      "Loss after 134192 examples: 0.049\n",
      "Loss after 134392 examples: 0.039\n",
      "Loss after 134592 examples: 0.053\n",
      "Loss after 134792 examples: 0.051\n",
      "Loss after 134992 examples: 0.058\n",
      "Loss after 135192 examples: 0.041\n",
      "Loss after 135392 examples: 0.039\n",
      "Loss after 135592 examples: 0.057\n",
      "Loss after 135792 examples: 0.043\n",
      "Loss after 135992 examples: 0.071\n",
      "Loss after 136192 examples: 0.040\n",
      "Loss after 136392 examples: 0.040\n",
      "Loss after 136592 examples: 0.035\n",
      "Loss after 136792 examples: 0.056\n",
      "Loss after 136992 examples: 0.053\n",
      "Loss after 137192 examples: 0.059\n",
      "Loss after 137392 examples: 0.072\n",
      "Loss after 137592 examples: 0.051\n",
      "Loss after 137792 examples: 0.047\n",
      "Loss after 137992 examples: 0.048\n",
      "Loss after 138192 examples: 0.051\n",
      "Loss after 138392 examples: 0.039\n",
      "Loss after 138592 examples: 0.066\n",
      "Loss after 138792 examples: 0.051\n",
      "Loss after 138992 examples: 0.075\n",
      "Loss after 139192 examples: 0.045\n",
      "Loss after 139392 examples: 0.044\n",
      "Loss after 139592 examples: 0.058\n",
      "Loss after 139792 examples: 0.038\n",
      "Loss after 139992 examples: 0.132\n",
      "Loss after 140192 examples: 0.058\n",
      "Loss after 140392 examples: 0.064\n",
      "Loss after 140592 examples: 0.055\n",
      "Loss after 140792 examples: 0.031\n",
      "Loss after 140992 examples: 0.039\n",
      "Loss after 141192 examples: 0.045\n",
      "Loss after 141392 examples: 0.039\n",
      "Loss after 141592 examples: 0.038\n",
      "Loss after 141792 examples: 0.050\n",
      "Loss after 141992 examples: 0.041\n",
      "Loss after 142192 examples: 0.064\n",
      "Loss after 142392 examples: 0.049\n",
      "Loss after 142592 examples: 0.085\n",
      "Loss after 142792 examples: 0.071\n",
      "Loss after 142992 examples: 0.051\n",
      "Loss after 143192 examples: 0.053\n",
      "Loss after 143392 examples: 0.065\n",
      "Loss after 143592 examples: 0.048\n",
      "Loss after 143792 examples: 0.040\n",
      "Loss after 143992 examples: 0.048\n",
      "Loss after 144192 examples: 0.054\n",
      "Loss after 144392 examples: 0.057\n",
      "Loss after 144592 examples: 0.040\n",
      "Loss after 144792 examples: 0.056\n",
      "Loss after 144992 examples: 0.047\n",
      "Loss after 145192 examples: 0.050\n",
      "Loss after 145392 examples: 0.076\n",
      "Loss after 145592 examples: 0.047\n",
      "Loss after 145792 examples: 0.050\n",
      "Loss after 145992 examples: 0.062\n",
      "Loss after 146192 examples: 0.052\n",
      "Loss after 146392 examples: 0.048\n",
      "Loss after 146592 examples: 0.055\n",
      "Loss after 146792 examples: 0.042\n",
      "Loss after 146992 examples: 0.071\n",
      "Loss after 147192 examples: 0.043\n",
      "Loss after 147392 examples: 0.052\n",
      "Loss after 147592 examples: 0.041\n",
      "Loss after 147792 examples: 0.043\n",
      "Loss after 147992 examples: 0.058\n",
      "Loss after 148192 examples: 0.052\n",
      "Loss after 148392 examples: 0.103\n",
      "Loss after 148592 examples: 0.052\n",
      "Loss after 148792 examples: 0.030\n",
      "Loss after 148992 examples: 0.038\n",
      "Loss after 149192 examples: 0.032\n",
      "Loss after 149392 examples: 0.045\n",
      "Loss after 149592 examples: 0.033\n",
      "Loss after 149792 examples: 0.038\n",
      "Loss after 149992 examples: 0.049\n",
      "Train Loss: 0.031\n",
      "Test Loss: 0.034\n",
      "[0.0001]\n",
      "Loss after 150192 examples: 0.055\n",
      "Loss after 150392 examples: 0.035\n",
      "Loss after 150592 examples: 0.081\n",
      "Loss after 150792 examples: 0.054\n",
      "Loss after 150992 examples: 0.031\n",
      "Loss after 151192 examples: 0.043\n",
      "Loss after 151392 examples: 0.046\n",
      "Loss after 151592 examples: 0.035\n",
      "Loss after 151792 examples: 0.060\n",
      "Loss after 151992 examples: 0.060\n",
      "Loss after 152192 examples: 0.044\n",
      "Loss after 152392 examples: 0.046\n",
      "Loss after 152592 examples: 0.051\n",
      "Loss after 152792 examples: 0.058\n",
      "Loss after 152992 examples: 0.061\n",
      "Loss after 153192 examples: 0.056\n",
      "Loss after 153392 examples: 0.037\n",
      "Loss after 153592 examples: 0.047\n",
      "Loss after 153792 examples: 0.037\n",
      "Loss after 153992 examples: 0.057\n",
      "Loss after 154192 examples: 0.045\n",
      "Loss after 154392 examples: 0.104\n",
      "Loss after 154592 examples: 0.049\n",
      "Loss after 154792 examples: 0.041\n",
      "Loss after 154992 examples: 0.053\n",
      "Loss after 155192 examples: 0.044\n",
      "Loss after 155392 examples: 0.048\n",
      "Loss after 155592 examples: 0.051\n",
      "Loss after 155792 examples: 0.035\n",
      "Loss after 155992 examples: 0.035\n",
      "Loss after 156192 examples: 0.058\n",
      "Loss after 156392 examples: 0.042\n",
      "Loss after 156592 examples: 0.052\n",
      "Loss after 156792 examples: 0.041\n",
      "Loss after 156992 examples: 0.052\n",
      "Loss after 157192 examples: 0.032\n",
      "Loss after 157392 examples: 0.031\n",
      "Loss after 157592 examples: 0.046\n",
      "Loss after 157792 examples: 0.054\n",
      "Loss after 157992 examples: 0.052\n",
      "Loss after 158192 examples: 0.049\n",
      "Loss after 158392 examples: 0.066\n",
      "Loss after 158592 examples: 0.040\n",
      "Loss after 158792 examples: 0.055\n",
      "Loss after 158992 examples: 0.031\n",
      "Loss after 159192 examples: 0.035\n",
      "Loss after 159392 examples: 0.033\n",
      "Loss after 159592 examples: 0.049\n",
      "Loss after 159792 examples: 0.064\n",
      "Loss after 159992 examples: 0.075\n",
      "Loss after 160192 examples: 0.038\n",
      "Loss after 160392 examples: 0.047\n",
      "Loss after 160592 examples: 0.038\n",
      "Loss after 160792 examples: 0.038\n",
      "Loss after 160992 examples: 0.050\n",
      "Loss after 161192 examples: 0.042\n",
      "Loss after 161392 examples: 0.063\n",
      "Loss after 161592 examples: 0.026\n",
      "Loss after 161792 examples: 0.030\n",
      "Loss after 161992 examples: 0.083\n",
      "Loss after 162192 examples: 0.039\n",
      "Loss after 162392 examples: 0.052\n",
      "Loss after 162592 examples: 0.053\n",
      "Loss after 162792 examples: 0.045\n",
      "Loss after 162992 examples: 0.045\n",
      "Loss after 163192 examples: 0.042\n",
      "Loss after 163392 examples: 0.043\n",
      "Loss after 163592 examples: 0.030\n",
      "Loss after 163792 examples: 0.048\n",
      "Loss after 163992 examples: 0.062\n",
      "Loss after 164192 examples: 0.046\n",
      "Loss after 164392 examples: 0.039\n",
      "Loss after 164592 examples: 0.047\n",
      "Loss after 164792 examples: 0.037\n",
      "Loss after 164992 examples: 0.030\n",
      "Loss after 165192 examples: 0.026\n",
      "Loss after 165392 examples: 0.055\n",
      "Loss after 165592 examples: 0.058\n",
      "Loss after 165792 examples: 0.062\n",
      "Loss after 165992 examples: 0.042\n",
      "Loss after 166192 examples: 0.059\n",
      "Loss after 166392 examples: 0.040\n",
      "Loss after 166592 examples: 0.038\n",
      "Loss after 166792 examples: 0.056\n",
      "Loss after 166992 examples: 0.054\n",
      "Loss after 167192 examples: 0.046\n",
      "Loss after 167392 examples: 0.038\n",
      "Loss after 167592 examples: 0.040\n",
      "Loss after 167792 examples: 0.052\n",
      "Loss after 167992 examples: 0.032\n",
      "Loss after 168192 examples: 0.030\n",
      "Loss after 168392 examples: 0.050\n",
      "Loss after 168592 examples: 0.032\n",
      "Loss after 168792 examples: 0.036\n",
      "Loss after 168992 examples: 0.040\n",
      "Loss after 169192 examples: 0.034\n",
      "Loss after 169392 examples: 0.055\n",
      "Loss after 169592 examples: 0.046\n",
      "Loss after 169792 examples: 0.047\n",
      "Loss after 169992 examples: 0.051\n",
      "Loss after 170192 examples: 0.058\n",
      "Loss after 170392 examples: 0.045\n",
      "Loss after 170592 examples: 0.044\n",
      "Loss after 170792 examples: 0.043\n",
      "Loss after 170992 examples: 0.052\n",
      "Loss after 171192 examples: 0.027\n",
      "Loss after 171392 examples: 0.047\n",
      "Loss after 171592 examples: 0.042\n",
      "Loss after 171792 examples: 0.037\n",
      "Loss after 171992 examples: 0.059\n",
      "Loss after 172192 examples: 0.027\n",
      "Loss after 172392 examples: 0.035\n",
      "Loss after 172592 examples: 0.022\n",
      "Loss after 172792 examples: 0.041\n",
      "Loss after 172992 examples: 0.044\n",
      "Loss after 173192 examples: 0.051\n",
      "Loss after 173392 examples: 0.048\n",
      "Loss after 173592 examples: 0.041\n",
      "Loss after 173792 examples: 0.034\n",
      "Loss after 173992 examples: 0.040\n",
      "Loss after 174192 examples: 0.029\n",
      "Loss after 174392 examples: 0.032\n",
      "Loss after 174592 examples: 0.033\n",
      "Loss after 174792 examples: 0.065\n",
      "Loss after 174992 examples: 0.021\n",
      "Loss after 175192 examples: 0.061\n",
      "Loss after 175392 examples: 0.049\n",
      "Loss after 175592 examples: 0.038\n",
      "Loss after 175792 examples: 0.040\n",
      "Loss after 175992 examples: 0.047\n",
      "Loss after 176192 examples: 0.053\n",
      "Loss after 176392 examples: 0.036\n",
      "Loss after 176592 examples: 0.036\n",
      "Loss after 176792 examples: 0.065\n",
      "Loss after 176992 examples: 0.035\n",
      "Loss after 177192 examples: 0.029\n",
      "Loss after 177392 examples: 0.058\n",
      "Loss after 177592 examples: 0.037\n",
      "Loss after 177792 examples: 0.034\n",
      "Loss after 177992 examples: 0.029\n",
      "Loss after 178192 examples: 0.052\n",
      "Loss after 178392 examples: 0.037\n",
      "Loss after 178592 examples: 0.057\n",
      "Loss after 178792 examples: 0.052\n",
      "Loss after 178992 examples: 0.025\n",
      "Loss after 179192 examples: 0.052\n",
      "Loss after 179392 examples: 0.033\n",
      "Loss after 179592 examples: 0.052\n",
      "Loss after 179792 examples: 0.046\n",
      "Loss after 179992 examples: 0.055\n",
      "Loss after 180192 examples: 0.030\n",
      "Loss after 180392 examples: 0.049\n",
      "Loss after 180592 examples: 0.072\n",
      "Loss after 180792 examples: 0.032\n",
      "Loss after 180992 examples: 0.028\n",
      "Loss after 181192 examples: 0.038\n",
      "Loss after 181392 examples: 0.035\n",
      "Loss after 181592 examples: 0.060\n",
      "Loss after 181792 examples: 0.028\n",
      "Loss after 181992 examples: 0.039\n",
      "Loss after 182192 examples: 0.051\n",
      "Loss after 182392 examples: 0.029\n",
      "Loss after 182592 examples: 0.065\n",
      "Loss after 182792 examples: 0.037\n",
      "Loss after 182992 examples: 0.054\n",
      "Loss after 183192 examples: 0.037\n",
      "Loss after 183392 examples: 0.042\n",
      "Loss after 183592 examples: 0.041\n",
      "Loss after 183792 examples: 0.036\n",
      "Loss after 183992 examples: 0.055\n",
      "Loss after 184192 examples: 0.041\n",
      "Loss after 184392 examples: 0.048\n",
      "Loss after 184592 examples: 0.047\n",
      "Loss after 184792 examples: 0.031\n",
      "Loss after 184992 examples: 0.035\n",
      "Loss after 185192 examples: 0.029\n",
      "Loss after 185392 examples: 0.051\n",
      "Loss after 185592 examples: 0.066\n",
      "Loss after 185792 examples: 0.034\n",
      "Loss after 185992 examples: 0.040\n",
      "Loss after 186192 examples: 0.036\n",
      "Loss after 186392 examples: 0.049\n",
      "Loss after 186592 examples: 0.029\n",
      "Loss after 186792 examples: 0.107\n",
      "Loss after 186992 examples: 0.060\n",
      "Loss after 187192 examples: 0.047\n",
      "Loss after 187392 examples: 0.046\n",
      "Loss after 187592 examples: 0.056\n",
      "Loss after 187792 examples: 0.034\n",
      "Loss after 187992 examples: 0.044\n",
      "Loss after 188192 examples: 0.031\n",
      "Loss after 188392 examples: 0.038\n",
      "Loss after 188592 examples: 0.051\n",
      "Loss after 188792 examples: 0.050\n",
      "Loss after 188992 examples: 0.038\n",
      "Loss after 189192 examples: 0.036\n",
      "Loss after 189392 examples: 0.042\n",
      "Loss after 189592 examples: 0.052\n",
      "Loss after 189792 examples: 0.046\n",
      "Loss after 189992 examples: 0.026\n",
      "Loss after 190192 examples: 0.039\n",
      "Loss after 190392 examples: 0.030\n",
      "Loss after 190592 examples: 0.047\n",
      "Loss after 190792 examples: 0.060\n",
      "Loss after 190992 examples: 0.046\n",
      "Loss after 191192 examples: 0.038\n",
      "Loss after 191392 examples: 0.034\n",
      "Loss after 191592 examples: 0.031\n",
      "Loss after 191792 examples: 0.040\n",
      "Loss after 191992 examples: 0.033\n",
      "Loss after 192192 examples: 0.036\n",
      "Loss after 192392 examples: 0.060\n",
      "Loss after 192592 examples: 0.038\n",
      "Loss after 192792 examples: 0.034\n",
      "Loss after 192992 examples: 0.035\n",
      "Loss after 193192 examples: 0.046\n",
      "Loss after 193392 examples: 0.048\n",
      "Loss after 193592 examples: 0.051\n",
      "Loss after 193792 examples: 0.041\n",
      "Loss after 193992 examples: 0.064\n",
      "Loss after 194192 examples: 0.039\n",
      "Loss after 194392 examples: 0.046\n",
      "Loss after 194592 examples: 0.049\n",
      "Loss after 194792 examples: 0.027\n",
      "Loss after 194992 examples: 0.071\n",
      "Loss after 195192 examples: 0.050\n",
      "Loss after 195392 examples: 0.046\n",
      "Loss after 195592 examples: 0.049\n",
      "Loss after 195792 examples: 0.027\n",
      "Loss after 195992 examples: 0.040\n",
      "Loss after 196192 examples: 0.047\n",
      "Loss after 196392 examples: 0.035\n",
      "Loss after 196592 examples: 0.123\n",
      "Loss after 196792 examples: 0.057\n",
      "Loss after 196992 examples: 0.048\n",
      "Loss after 197192 examples: 0.036\n",
      "Loss after 197392 examples: 0.037\n",
      "Loss after 197592 examples: 0.031\n",
      "Loss after 197792 examples: 0.047\n",
      "Loss after 197992 examples: 0.035\n",
      "Loss after 198192 examples: 0.045\n",
      "Loss after 198392 examples: 0.066\n",
      "Loss after 198592 examples: 0.026\n",
      "Loss after 198792 examples: 0.024\n",
      "Loss after 198992 examples: 0.055\n",
      "Loss after 199192 examples: 0.052\n",
      "Loss after 199392 examples: 0.039\n",
      "Loss after 199592 examples: 0.029\n",
      "Loss after 199792 examples: 0.053\n",
      "Loss after 199992 examples: 0.042\n",
      "Train Loss: 0.024\n",
      "Test Loss: 0.028\n",
      "[0.0001]\n",
      "Loss after 200192 examples: 0.022\n",
      "Loss after 200392 examples: 0.049\n",
      "Loss after 200592 examples: 0.045\n",
      "Loss after 200792 examples: 0.034\n",
      "Loss after 200992 examples: 0.039\n",
      "Loss after 201192 examples: 0.039\n",
      "Loss after 201392 examples: 0.043\n",
      "Loss after 201592 examples: 0.031\n",
      "Loss after 201792 examples: 0.031\n",
      "Loss after 201992 examples: 0.034\n",
      "Loss after 202192 examples: 0.034\n",
      "Loss after 202392 examples: 0.029\n",
      "Loss after 202592 examples: 0.053\n",
      "Loss after 202792 examples: 0.045\n",
      "Loss after 202992 examples: 0.027\n",
      "Loss after 203192 examples: 0.045\n",
      "Loss after 203392 examples: 0.052\n",
      "Loss after 203592 examples: 0.034\n",
      "Loss after 203792 examples: 0.039\n",
      "Loss after 203992 examples: 0.043\n",
      "Loss after 204192 examples: 0.027\n",
      "Loss after 204392 examples: 0.034\n",
      "Loss after 204592 examples: 0.049\n",
      "Loss after 204792 examples: 0.030\n",
      "Loss after 204992 examples: 0.032\n",
      "Loss after 205192 examples: 0.064\n",
      "Loss after 205392 examples: 0.034\n",
      "Loss after 205592 examples: 0.034\n",
      "Loss after 205792 examples: 0.055\n",
      "Loss after 205992 examples: 0.050\n",
      "Loss after 206192 examples: 0.040\n",
      "Loss after 206392 examples: 0.043\n",
      "Loss after 206592 examples: 0.027\n",
      "Loss after 206792 examples: 0.035\n",
      "Loss after 206992 examples: 0.048\n",
      "Loss after 207192 examples: 0.052\n",
      "Loss after 207392 examples: 0.040\n",
      "Loss after 207592 examples: 0.039\n",
      "Loss after 207792 examples: 0.049\n",
      "Loss after 207992 examples: 0.049\n",
      "Loss after 208192 examples: 0.034\n",
      "Loss after 208392 examples: 0.042\n",
      "Loss after 208592 examples: 0.033\n",
      "Loss after 208792 examples: 0.034\n",
      "Loss after 208992 examples: 0.043\n",
      "Loss after 209192 examples: 0.038\n",
      "Loss after 209392 examples: 0.035\n",
      "Loss after 209592 examples: 0.030\n",
      "Loss after 209792 examples: 0.035\n",
      "Loss after 209992 examples: 0.032\n",
      "Loss after 210192 examples: 0.030\n",
      "Loss after 210392 examples: 0.048\n",
      "Loss after 210592 examples: 0.043\n",
      "Loss after 210792 examples: 0.035\n",
      "Loss after 210992 examples: 0.056\n",
      "Loss after 211192 examples: 0.051\n",
      "Loss after 211392 examples: 0.045\n",
      "Loss after 211592 examples: 0.055\n",
      "Loss after 211792 examples: 0.033\n",
      "Loss after 211992 examples: 0.029\n",
      "Loss after 212192 examples: 0.043\n",
      "Loss after 212392 examples: 0.035\n",
      "Loss after 212592 examples: 0.040\n",
      "Loss after 212792 examples: 0.029\n",
      "Loss after 212992 examples: 0.038\n",
      "Loss after 213192 examples: 0.031\n",
      "Loss after 213392 examples: 0.033\n",
      "Loss after 213592 examples: 0.035\n",
      "Loss after 213792 examples: 0.032\n",
      "Loss after 213992 examples: 0.033\n",
      "Loss after 214192 examples: 0.035\n",
      "Loss after 214392 examples: 0.042\n",
      "Loss after 214592 examples: 0.025\n",
      "Loss after 214792 examples: 0.030\n",
      "Loss after 214992 examples: 0.026\n",
      "Loss after 215192 examples: 0.043\n",
      "Loss after 215392 examples: 0.036\n",
      "Loss after 215592 examples: 0.026\n",
      "Loss after 215792 examples: 0.076\n",
      "Loss after 215992 examples: 0.053\n",
      "Loss after 216192 examples: 0.039\n",
      "Loss after 216392 examples: 0.055\n",
      "Loss after 216592 examples: 0.034\n",
      "Loss after 216792 examples: 0.034\n",
      "Loss after 216992 examples: 0.046\n",
      "Loss after 217192 examples: 0.044\n",
      "Loss after 217392 examples: 0.042\n",
      "Loss after 217592 examples: 0.023\n",
      "Loss after 217792 examples: 0.024\n",
      "Loss after 217992 examples: 0.041\n",
      "Loss after 218192 examples: 0.045\n",
      "Loss after 218392 examples: 0.046\n",
      "Loss after 218592 examples: 0.031\n",
      "Loss after 218792 examples: 0.029\n",
      "Loss after 218992 examples: 0.042\n",
      "Loss after 219192 examples: 0.032\n",
      "Loss after 219392 examples: 0.033\n",
      "Loss after 219592 examples: 0.044\n",
      "Loss after 219792 examples: 0.047\n",
      "Loss after 219992 examples: 0.021\n",
      "Loss after 220192 examples: 0.042\n",
      "Loss after 220392 examples: 0.033\n",
      "Loss after 220592 examples: 0.033\n",
      "Loss after 220792 examples: 0.031\n",
      "Loss after 220992 examples: 0.080\n",
      "Loss after 221192 examples: 0.024\n",
      "Loss after 221392 examples: 0.062\n",
      "Loss after 221592 examples: 0.034\n",
      "Loss after 221792 examples: 0.103\n",
      "Loss after 221992 examples: 0.033\n",
      "Loss after 222192 examples: 0.033\n",
      "Loss after 222392 examples: 0.029\n",
      "Loss after 222592 examples: 0.047\n",
      "Loss after 222792 examples: 0.034\n",
      "Loss after 222992 examples: 0.030\n",
      "Loss after 223192 examples: 0.043\n",
      "Loss after 223392 examples: 0.033\n",
      "Loss after 223592 examples: 0.057\n",
      "Loss after 223792 examples: 0.032\n",
      "Loss after 223992 examples: 0.046\n",
      "Loss after 224192 examples: 0.052\n",
      "Loss after 224392 examples: 0.034\n",
      "Loss after 224592 examples: 0.047\n",
      "Loss after 224792 examples: 0.042\n",
      "Loss after 224992 examples: 0.059\n",
      "Loss after 225192 examples: 0.035\n",
      "Loss after 225392 examples: 0.077\n",
      "Loss after 225592 examples: 0.043\n",
      "Loss after 225792 examples: 0.033\n",
      "Loss after 225992 examples: 0.057\n",
      "Loss after 226192 examples: 0.022\n",
      "Loss after 226392 examples: 0.022\n",
      "Loss after 226592 examples: 0.035\n",
      "Loss after 226792 examples: 0.044\n",
      "Loss after 226992 examples: 0.044\n",
      "Loss after 227192 examples: 0.022\n",
      "Loss after 227392 examples: 0.032\n",
      "Loss after 227592 examples: 0.067\n",
      "Loss after 227792 examples: 0.041\n",
      "Loss after 227992 examples: 0.061\n",
      "Loss after 228192 examples: 0.037\n",
      "Loss after 228392 examples: 0.042\n",
      "Loss after 228592 examples: 0.045\n",
      "Loss after 228792 examples: 0.038\n",
      "Loss after 228992 examples: 0.034\n",
      "Loss after 229192 examples: 0.045\n",
      "Loss after 229392 examples: 0.043\n",
      "Loss after 229592 examples: 0.035\n",
      "Loss after 229792 examples: 0.031\n",
      "Loss after 229992 examples: 0.035\n",
      "Loss after 230192 examples: 0.048\n",
      "Loss after 230392 examples: 0.041\n",
      "Loss after 230592 examples: 0.030\n",
      "Loss after 230792 examples: 0.026\n",
      "Loss after 230992 examples: 0.048\n",
      "Loss after 231192 examples: 0.044\n",
      "Loss after 231392 examples: 0.019\n",
      "Loss after 231592 examples: 0.033\n",
      "Loss after 231792 examples: 0.043\n",
      "Loss after 231992 examples: 0.024\n",
      "Loss after 232192 examples: 0.017\n",
      "Loss after 232392 examples: 0.026\n",
      "Loss after 232592 examples: 0.045\n",
      "Loss after 232792 examples: 0.035\n",
      "Loss after 232992 examples: 0.028\n",
      "Loss after 233192 examples: 0.035\n",
      "Loss after 233392 examples: 0.030\n",
      "Loss after 233592 examples: 0.046\n",
      "Loss after 233792 examples: 0.025\n",
      "Loss after 233992 examples: 0.029\n",
      "Loss after 234192 examples: 0.031\n",
      "Loss after 234392 examples: 0.038\n",
      "Loss after 234592 examples: 0.036\n",
      "Loss after 234792 examples: 0.033\n",
      "Loss after 234992 examples: 0.034\n",
      "Loss after 235192 examples: 0.045\n",
      "Loss after 235392 examples: 0.035\n",
      "Loss after 235592 examples: 0.029\n",
      "Loss after 235792 examples: 0.034\n",
      "Loss after 235992 examples: 0.025\n",
      "Loss after 236192 examples: 0.027\n",
      "Loss after 236392 examples: 0.030\n",
      "Loss after 236592 examples: 0.030\n",
      "Loss after 236792 examples: 0.034\n",
      "Loss after 236992 examples: 0.041\n",
      "Loss after 237192 examples: 0.027\n",
      "Loss after 237392 examples: 0.051\n",
      "Loss after 237592 examples: 0.045\n",
      "Loss after 237792 examples: 0.029\n",
      "Loss after 237992 examples: 0.025\n",
      "Loss after 238192 examples: 0.040\n",
      "Loss after 238392 examples: 0.055\n",
      "Loss after 238592 examples: 0.049\n",
      "Loss after 238792 examples: 0.044\n",
      "Loss after 238992 examples: 0.049\n",
      "Loss after 239192 examples: 0.037\n",
      "Loss after 239392 examples: 0.052\n",
      "Loss after 239592 examples: 0.046\n",
      "Loss after 239792 examples: 0.067\n",
      "Loss after 239992 examples: 0.046\n",
      "Loss after 240192 examples: 0.049\n",
      "Loss after 240392 examples: 0.026\n",
      "Loss after 240592 examples: 0.024\n",
      "Loss after 240792 examples: 0.035\n",
      "Loss after 240992 examples: 0.044\n",
      "Loss after 241192 examples: 0.045\n",
      "Loss after 241392 examples: 0.045\n",
      "Loss after 241592 examples: 0.033\n",
      "Loss after 241792 examples: 0.029\n",
      "Loss after 241992 examples: 0.022\n",
      "Loss after 242192 examples: 0.048\n",
      "Loss after 242392 examples: 0.044\n",
      "Loss after 242592 examples: 0.041\n",
      "Loss after 242792 examples: 0.052\n",
      "Loss after 242992 examples: 0.032\n",
      "Loss after 243192 examples: 0.031\n",
      "Loss after 243392 examples: 0.044\n",
      "Loss after 243592 examples: 0.032\n",
      "Loss after 243792 examples: 0.031\n",
      "Loss after 243992 examples: 0.036\n",
      "Loss after 244192 examples: 0.033\n",
      "Loss after 244392 examples: 0.031\n",
      "Loss after 244592 examples: 0.044\n",
      "Loss after 244792 examples: 0.026\n",
      "Loss after 244992 examples: 0.036\n",
      "Loss after 245192 examples: 0.035\n",
      "Loss after 245392 examples: 0.026\n",
      "Loss after 245592 examples: 0.029\n",
      "Loss after 245792 examples: 0.023\n",
      "Loss after 245992 examples: 0.040\n",
      "Loss after 246192 examples: 0.043\n",
      "Loss after 246392 examples: 0.030\n",
      "Loss after 246592 examples: 0.036\n",
      "Loss after 246792 examples: 0.046\n",
      "Loss after 246992 examples: 0.039\n",
      "Loss after 247192 examples: 0.041\n",
      "Loss after 247392 examples: 0.028\n",
      "Loss after 247592 examples: 0.030\n",
      "Loss after 247792 examples: 0.040\n",
      "Loss after 247992 examples: 0.047\n",
      "Loss after 248192 examples: 0.027\n",
      "Loss after 248392 examples: 0.042\n",
      "Loss after 248592 examples: 0.022\n",
      "Loss after 248792 examples: 0.043\n",
      "Loss after 248992 examples: 0.053\n",
      "Loss after 249192 examples: 0.042\n",
      "Loss after 249392 examples: 0.035\n",
      "Loss after 249592 examples: 0.022\n",
      "Loss after 249792 examples: 0.044\n",
      "Loss after 249992 examples: 0.030\n",
      "Train Loss: 0.020\n",
      "Test Loss: 0.024\n",
      "[0.0001]\n",
      "Loss after 250192 examples: 0.032\n",
      "Loss after 250392 examples: 0.032\n",
      "Loss after 250592 examples: 0.024\n",
      "Loss after 250792 examples: 0.038\n",
      "Loss after 250992 examples: 0.027\n",
      "Loss after 251192 examples: 0.023\n",
      "Loss after 251392 examples: 0.037\n",
      "Loss after 251592 examples: 0.017\n",
      "Loss after 251792 examples: 0.046\n",
      "Loss after 251992 examples: 0.039\n",
      "Loss after 252192 examples: 0.025\n",
      "Loss after 252392 examples: 0.023\n",
      "Loss after 252592 examples: 0.024\n",
      "Loss after 252792 examples: 0.040\n",
      "Loss after 252992 examples: 0.033\n",
      "Loss after 253192 examples: 0.020\n",
      "Loss after 253392 examples: 0.042\n",
      "Loss after 253592 examples: 0.021\n",
      "Loss after 253792 examples: 0.045\n",
      "Loss after 253992 examples: 0.020\n",
      "Loss after 254192 examples: 0.027\n",
      "Loss after 254392 examples: 0.034\n",
      "Loss after 254592 examples: 0.035\n",
      "Loss after 254792 examples: 0.026\n",
      "Loss after 254992 examples: 0.027\n",
      "Loss after 255192 examples: 0.037\n",
      "Loss after 255392 examples: 0.024\n",
      "Loss after 255592 examples: 0.023\n",
      "Loss after 255792 examples: 0.027\n",
      "Loss after 255992 examples: 0.039\n",
      "Loss after 256192 examples: 0.033\n",
      "Loss after 256392 examples: 0.033\n",
      "Loss after 256592 examples: 0.027\n",
      "Loss after 256792 examples: 0.034\n",
      "Loss after 256992 examples: 0.028\n",
      "Loss after 257192 examples: 0.027\n",
      "Loss after 257392 examples: 0.037\n",
      "Loss after 257592 examples: 0.039\n",
      "Loss after 257792 examples: 0.038\n",
      "Loss after 257992 examples: 0.042\n",
      "Loss after 258192 examples: 0.037\n",
      "Loss after 258392 examples: 0.043\n",
      "Loss after 258592 examples: 0.035\n",
      "Loss after 258792 examples: 0.024\n",
      "Loss after 258992 examples: 0.032\n",
      "Loss after 259192 examples: 0.048\n",
      "Loss after 259392 examples: 0.028\n",
      "Loss after 259592 examples: 0.026\n",
      "Loss after 259792 examples: 0.041\n",
      "Loss after 259992 examples: 0.036\n",
      "Loss after 260192 examples: 0.024\n",
      "Loss after 260392 examples: 0.033\n",
      "Loss after 260592 examples: 0.040\n",
      "Loss after 260792 examples: 0.028\n",
      "Loss after 260992 examples: 0.056\n",
      "Loss after 261192 examples: 0.046\n",
      "Loss after 261392 examples: 0.026\n",
      "Loss after 261592 examples: 0.027\n",
      "Loss after 261792 examples: 0.036\n",
      "Loss after 261992 examples: 0.031\n",
      "Loss after 262192 examples: 0.044\n",
      "Loss after 262392 examples: 0.026\n",
      "Loss after 262592 examples: 0.029\n",
      "Loss after 262792 examples: 0.021\n",
      "Loss after 262992 examples: 0.047\n",
      "Loss after 263192 examples: 0.046\n",
      "Loss after 263392 examples: 0.040\n",
      "Loss after 263592 examples: 0.051\n",
      "Loss after 263792 examples: 0.054\n",
      "Loss after 263992 examples: 0.021\n",
      "Loss after 264192 examples: 0.048\n",
      "Loss after 264392 examples: 0.039\n",
      "Loss after 264592 examples: 0.038\n",
      "Loss after 264792 examples: 0.031\n",
      "Loss after 264992 examples: 0.019\n",
      "Loss after 265192 examples: 0.029\n",
      "Loss after 265392 examples: 0.030\n",
      "Loss after 265592 examples: 0.029\n",
      "Loss after 265792 examples: 0.018\n",
      "Loss after 265992 examples: 0.045\n",
      "Loss after 266192 examples: 0.021\n",
      "Loss after 266392 examples: 0.045\n",
      "Loss after 266592 examples: 0.023\n",
      "Loss after 266792 examples: 0.033\n",
      "Loss after 266992 examples: 0.043\n",
      "Loss after 267192 examples: 0.028\n",
      "Loss after 267392 examples: 0.037\n",
      "Loss after 267592 examples: 0.022\n",
      "Loss after 267792 examples: 0.024\n",
      "Loss after 267992 examples: 0.038\n",
      "Loss after 268192 examples: 0.020\n",
      "Loss after 268392 examples: 0.030\n",
      "Loss after 268592 examples: 0.024\n",
      "Loss after 268792 examples: 0.046\n",
      "Loss after 268992 examples: 0.036\n",
      "Loss after 269192 examples: 0.043\n",
      "Loss after 269392 examples: 0.028\n",
      "Loss after 269592 examples: 0.036\n",
      "Loss after 269792 examples: 0.025\n",
      "Loss after 269992 examples: 0.034\n",
      "Loss after 270192 examples: 0.033\n",
      "Loss after 270392 examples: 0.027\n",
      "Loss after 270592 examples: 0.029\n",
      "Loss after 270792 examples: 0.027\n",
      "Loss after 270992 examples: 0.035\n",
      "Loss after 271192 examples: 0.039\n",
      "Loss after 271392 examples: 0.043\n",
      "Loss after 271592 examples: 0.030\n",
      "Loss after 271792 examples: 0.017\n",
      "Loss after 271992 examples: 0.042\n",
      "Loss after 272192 examples: 0.026\n",
      "Loss after 272392 examples: 0.022\n",
      "Loss after 272592 examples: 0.044\n",
      "Loss after 272792 examples: 0.022\n",
      "Loss after 272992 examples: 0.030\n",
      "Loss after 273192 examples: 0.024\n",
      "Loss after 273392 examples: 0.033\n",
      "Loss after 273592 examples: 0.028\n",
      "Loss after 273792 examples: 0.037\n",
      "Loss after 273992 examples: 0.032\n",
      "Loss after 274192 examples: 0.039\n",
      "Loss after 274392 examples: 0.034\n",
      "Loss after 274592 examples: 0.033\n",
      "Loss after 274792 examples: 0.017\n",
      "Loss after 274992 examples: 0.032\n",
      "Loss after 275192 examples: 0.056\n",
      "Loss after 275392 examples: 0.040\n",
      "Loss after 275592 examples: 0.016\n",
      "Loss after 275792 examples: 0.029\n",
      "Loss after 275992 examples: 0.029\n",
      "Loss after 276192 examples: 0.027\n",
      "Loss after 276392 examples: 0.032\n",
      "Loss after 276592 examples: 0.014\n",
      "Loss after 276792 examples: 0.025\n",
      "Loss after 276992 examples: 0.028\n",
      "Loss after 277192 examples: 0.033\n",
      "Loss after 277392 examples: 0.018\n",
      "Loss after 277592 examples: 0.037\n",
      "Loss after 277792 examples: 0.022\n",
      "Loss after 277992 examples: 0.029\n",
      "Loss after 278192 examples: 0.029\n",
      "Loss after 278392 examples: 0.028\n",
      "Loss after 278592 examples: 0.032\n",
      "Loss after 278792 examples: 0.017\n",
      "Loss after 278992 examples: 0.033\n",
      "Loss after 279192 examples: 0.038\n",
      "Loss after 279392 examples: 0.026\n",
      "Loss after 279592 examples: 0.032\n",
      "Loss after 279792 examples: 0.040\n",
      "Loss after 279992 examples: 0.030\n",
      "Loss after 280192 examples: 0.018\n",
      "Loss after 280392 examples: 0.034\n",
      "Loss after 280592 examples: 0.041\n",
      "Loss after 280792 examples: 0.030\n",
      "Loss after 280992 examples: 0.045\n",
      "Loss after 281192 examples: 0.045\n",
      "Loss after 281392 examples: 0.019\n",
      "Loss after 281592 examples: 0.045\n",
      "Loss after 281792 examples: 0.020\n",
      "Loss after 281992 examples: 0.048\n",
      "Loss after 282192 examples: 0.028\n",
      "Loss after 282392 examples: 0.033\n",
      "Loss after 282592 examples: 0.020\n",
      "Loss after 282792 examples: 0.035\n",
      "Loss after 282992 examples: 0.035\n",
      "Loss after 283192 examples: 0.030\n",
      "Loss after 283392 examples: 0.026\n",
      "Loss after 283592 examples: 0.037\n",
      "Loss after 283792 examples: 0.018\n",
      "Loss after 283992 examples: 0.021\n",
      "Loss after 284192 examples: 0.051\n",
      "Loss after 284392 examples: 0.040\n",
      "Loss after 284592 examples: 0.016\n",
      "Loss after 284792 examples: 0.026\n",
      "Loss after 284992 examples: 0.030\n",
      "Loss after 285192 examples: 0.035\n",
      "Loss after 285392 examples: 0.024\n",
      "Loss after 285592 examples: 0.044\n",
      "Loss after 285792 examples: 0.022\n",
      "Loss after 285992 examples: 0.032\n",
      "Loss after 286192 examples: 0.051\n",
      "Loss after 286392 examples: 0.031\n",
      "Loss after 286592 examples: 0.046\n",
      "Loss after 286792 examples: 0.025\n",
      "Loss after 286992 examples: 0.025\n",
      "Loss after 287192 examples: 0.039\n",
      "Loss after 287392 examples: 0.028\n",
      "Loss after 287592 examples: 0.026\n",
      "Loss after 287792 examples: 0.013\n",
      "Loss after 287992 examples: 0.036\n",
      "Loss after 288192 examples: 0.036\n",
      "Loss after 288392 examples: 0.033\n",
      "Loss after 288592 examples: 0.021\n",
      "Loss after 288792 examples: 0.032\n",
      "Loss after 288992 examples: 0.031\n",
      "Loss after 289192 examples: 0.027\n",
      "Loss after 289392 examples: 0.045\n",
      "Loss after 289592 examples: 0.027\n",
      "Loss after 289792 examples: 0.053\n",
      "Loss after 289992 examples: 0.049\n",
      "Loss after 290192 examples: 0.070\n",
      "Loss after 290392 examples: 0.031\n",
      "Loss after 290592 examples: 0.030\n",
      "Loss after 290792 examples: 0.027\n",
      "Loss after 290992 examples: 0.047\n",
      "Loss after 291192 examples: 0.055\n",
      "Loss after 291392 examples: 0.032\n",
      "Loss after 291592 examples: 0.035\n",
      "Loss after 291792 examples: 0.025\n",
      "Loss after 291992 examples: 0.020\n",
      "Loss after 292192 examples: 0.029\n",
      "Loss after 292392 examples: 0.024\n",
      "Loss after 292592 examples: 0.023\n",
      "Loss after 292792 examples: 0.041\n",
      "Loss after 292992 examples: 0.022\n",
      "Loss after 293192 examples: 0.028\n",
      "Loss after 293392 examples: 0.022\n",
      "Loss after 293592 examples: 0.034\n",
      "Loss after 293792 examples: 0.029\n",
      "Loss after 293992 examples: 0.039\n",
      "Loss after 294192 examples: 0.032\n",
      "Loss after 294392 examples: 0.034\n",
      "Loss after 294592 examples: 0.043\n",
      "Loss after 294792 examples: 0.023\n",
      "Loss after 294992 examples: 0.042\n",
      "Loss after 295192 examples: 0.032\n",
      "Loss after 295392 examples: 0.049\n",
      "Loss after 295592 examples: 0.021\n",
      "Loss after 295792 examples: 0.033\n",
      "Loss after 295992 examples: 0.025\n",
      "Loss after 296192 examples: 0.042\n",
      "Loss after 296392 examples: 0.032\n",
      "Loss after 296592 examples: 0.029\n",
      "Loss after 296792 examples: 0.031\n",
      "Loss after 296992 examples: 0.026\n",
      "Loss after 297192 examples: 0.042\n",
      "Loss after 297392 examples: 0.046\n",
      "Loss after 297592 examples: 0.019\n",
      "Loss after 297792 examples: 0.042\n",
      "Loss after 297992 examples: 0.026\n",
      "Loss after 298192 examples: 0.021\n",
      "Loss after 298392 examples: 0.038\n",
      "Loss after 298592 examples: 0.046\n",
      "Loss after 298792 examples: 0.029\n",
      "Loss after 298992 examples: 0.023\n",
      "Loss after 299192 examples: 0.044\n",
      "Loss after 299392 examples: 0.042\n",
      "Loss after 299592 examples: 0.037\n",
      "Loss after 299792 examples: 0.037\n",
      "Loss after 299992 examples: 0.021\n",
      "Train Loss: 0.016\n",
      "Test Loss: 0.021\n",
      "[0.0001]\n",
      "Loss after 300192 examples: 0.032\n",
      "Loss after 300392 examples: 0.031\n",
      "Loss after 300592 examples: 0.028\n",
      "Loss after 300792 examples: 0.032\n",
      "Loss after 300992 examples: 0.014\n",
      "Loss after 301192 examples: 0.040\n",
      "Loss after 301392 examples: 0.029\n",
      "Loss after 301592 examples: 0.039\n",
      "Loss after 301792 examples: 0.029\n",
      "Loss after 301992 examples: 0.042\n",
      "Loss after 302192 examples: 0.022\n",
      "Loss after 302392 examples: 0.031\n",
      "Loss after 302592 examples: 0.024\n",
      "Loss after 302792 examples: 0.038\n",
      "Loss after 302992 examples: 0.039\n",
      "Loss after 303192 examples: 0.032\n",
      "Loss after 303392 examples: 0.023\n",
      "Loss after 303592 examples: 0.031\n",
      "Loss after 303792 examples: 0.021\n",
      "Loss after 303992 examples: 0.041\n",
      "Loss after 304192 examples: 0.034\n",
      "Loss after 304392 examples: 0.017\n",
      "Loss after 304592 examples: 0.022\n",
      "Loss after 304792 examples: 0.024\n",
      "Loss after 304992 examples: 0.032\n",
      "Loss after 305192 examples: 0.030\n",
      "Loss after 305392 examples: 0.016\n",
      "Loss after 305592 examples: 0.022\n",
      "Loss after 305792 examples: 0.033\n",
      "Loss after 305992 examples: 0.032\n",
      "Loss after 306192 examples: 0.018\n",
      "Loss after 306392 examples: 0.030\n",
      "Loss after 306592 examples: 0.054\n",
      "Loss after 306792 examples: 0.029\n",
      "Loss after 306992 examples: 0.032\n",
      "Loss after 307192 examples: 0.043\n",
      "Loss after 307392 examples: 0.022\n",
      "Loss after 307592 examples: 0.041\n",
      "Loss after 307792 examples: 0.068\n",
      "Loss after 307992 examples: 0.029\n",
      "Loss after 308192 examples: 0.051\n",
      "Loss after 308392 examples: 0.025\n",
      "Loss after 308592 examples: 0.029\n",
      "Loss after 308792 examples: 0.023\n",
      "Loss after 308992 examples: 0.039\n",
      "Loss after 309192 examples: 0.036\n",
      "Loss after 309392 examples: 0.037\n",
      "Loss after 309592 examples: 0.023\n",
      "Loss after 309792 examples: 0.038\n",
      "Loss after 309992 examples: 0.029\n",
      "Loss after 310192 examples: 0.020\n",
      "Loss after 310392 examples: 0.022\n",
      "Loss after 310592 examples: 0.030\n",
      "Loss after 310792 examples: 0.026\n",
      "Loss after 310992 examples: 0.036\n",
      "Loss after 311192 examples: 0.033\n",
      "Loss after 311392 examples: 0.019\n",
      "Loss after 311592 examples: 0.032\n",
      "Loss after 311792 examples: 0.028\n",
      "Loss after 311992 examples: 0.033\n",
      "Loss after 312192 examples: 0.025\n",
      "Loss after 312392 examples: 0.026\n",
      "Loss after 312592 examples: 0.055\n",
      "Loss after 312792 examples: 0.018\n",
      "Loss after 312992 examples: 0.030\n",
      "Loss after 313192 examples: 0.017\n",
      "Loss after 313392 examples: 0.030\n",
      "Loss after 313592 examples: 0.029\n",
      "Loss after 313792 examples: 0.023\n",
      "Loss after 313992 examples: 0.022\n",
      "Loss after 314192 examples: 0.030\n",
      "Loss after 314392 examples: 0.028\n",
      "Loss after 314592 examples: 0.024\n",
      "Loss after 314792 examples: 0.040\n",
      "Loss after 314992 examples: 0.024\n",
      "Loss after 315192 examples: 0.031\n",
      "Loss after 315392 examples: 0.043\n",
      "Loss after 315592 examples: 0.038\n",
      "Loss after 315792 examples: 0.022\n",
      "Loss after 315992 examples: 0.020\n",
      "Loss after 316192 examples: 0.034\n",
      "Loss after 316392 examples: 0.027\n",
      "Loss after 316592 examples: 0.019\n",
      "Loss after 316792 examples: 0.022\n",
      "Loss after 316992 examples: 0.022\n",
      "Loss after 317192 examples: 0.022\n",
      "Loss after 317392 examples: 0.039\n",
      "Loss after 317592 examples: 0.022\n",
      "Loss after 317792 examples: 0.025\n",
      "Loss after 317992 examples: 0.026\n",
      "Loss after 318192 examples: 0.027\n",
      "Loss after 318392 examples: 0.043\n",
      "Loss after 318592 examples: 0.041\n",
      "Loss after 318792 examples: 0.024\n",
      "Loss after 318992 examples: 0.022\n",
      "Loss after 319192 examples: 0.016\n",
      "Loss after 319392 examples: 0.023\n",
      "Loss after 319592 examples: 0.037\n",
      "Loss after 319792 examples: 0.037\n",
      "Loss after 319992 examples: 0.029\n",
      "Loss after 320192 examples: 0.034\n",
      "Loss after 320392 examples: 0.033\n",
      "Loss after 320592 examples: 0.017\n",
      "Loss after 320792 examples: 0.035\n",
      "Loss after 320992 examples: 0.028\n",
      "Loss after 321192 examples: 0.036\n",
      "Loss after 321392 examples: 0.080\n",
      "Loss after 321592 examples: 0.018\n",
      "Loss after 321792 examples: 0.022\n",
      "Loss after 321992 examples: 0.032\n",
      "Loss after 322192 examples: 0.041\n",
      "Loss after 322392 examples: 0.016\n",
      "Loss after 322592 examples: 0.034\n",
      "Loss after 322792 examples: 0.027\n",
      "Loss after 322992 examples: 0.043\n",
      "Loss after 323192 examples: 0.037\n",
      "Loss after 323392 examples: 0.033\n",
      "Loss after 323592 examples: 0.022\n",
      "Loss after 323792 examples: 0.025\n",
      "Loss after 323992 examples: 0.031\n",
      "Loss after 324192 examples: 0.030\n",
      "Loss after 324392 examples: 0.025\n",
      "Loss after 324592 examples: 0.023\n",
      "Loss after 324792 examples: 0.020\n",
      "Loss after 324992 examples: 0.033\n",
      "Loss after 325192 examples: 0.032\n",
      "Loss after 325392 examples: 0.040\n",
      "Loss after 325592 examples: 0.043\n",
      "Loss after 325792 examples: 0.028\n",
      "Loss after 325992 examples: 0.015\n",
      "Loss after 326192 examples: 0.029\n",
      "Loss after 326392 examples: 0.025\n",
      "Loss after 326592 examples: 0.018\n",
      "Loss after 326792 examples: 0.034\n",
      "Loss after 326992 examples: 0.018\n",
      "Loss after 327192 examples: 0.024\n",
      "Loss after 327392 examples: 0.048\n",
      "Loss after 327592 examples: 0.025\n",
      "Loss after 327792 examples: 0.023\n",
      "Loss after 327992 examples: 0.024\n",
      "Loss after 328192 examples: 0.019\n",
      "Loss after 328392 examples: 0.027\n",
      "Loss after 328592 examples: 0.025\n",
      "Loss after 328792 examples: 0.035\n",
      "Loss after 328992 examples: 0.027\n",
      "Loss after 329192 examples: 0.043\n",
      "Loss after 329392 examples: 0.033\n",
      "Loss after 329592 examples: 0.027\n",
      "Loss after 329792 examples: 0.030\n",
      "Loss after 329992 examples: 0.023\n",
      "Loss after 330192 examples: 0.049\n",
      "Loss after 330392 examples: 0.030\n",
      "Loss after 330592 examples: 0.027\n",
      "Loss after 330792 examples: 0.016\n",
      "Loss after 330992 examples: 0.024\n",
      "Loss after 331192 examples: 0.038\n",
      "Loss after 331392 examples: 0.037\n",
      "Loss after 331592 examples: 0.016\n",
      "Loss after 331792 examples: 0.033\n",
      "Loss after 331992 examples: 0.033\n",
      "Loss after 332192 examples: 0.024\n",
      "Loss after 332392 examples: 0.024\n",
      "Loss after 332592 examples: 0.045\n",
      "Loss after 332792 examples: 0.029\n",
      "Loss after 332992 examples: 0.023\n",
      "Loss after 333192 examples: 0.027\n",
      "Loss after 333392 examples: 0.032\n",
      "Loss after 333592 examples: 0.037\n",
      "Loss after 333792 examples: 0.037\n",
      "Loss after 333992 examples: 0.023\n",
      "Loss after 334192 examples: 0.026\n",
      "Loss after 334392 examples: 0.016\n",
      "Loss after 334592 examples: 0.019\n",
      "Loss after 334792 examples: 0.021\n",
      "Loss after 334992 examples: 0.020\n",
      "Loss after 335192 examples: 0.024\n",
      "Loss after 335392 examples: 0.012\n",
      "Loss after 335592 examples: 0.031\n",
      "Loss after 335792 examples: 0.030\n",
      "Loss after 335992 examples: 0.028\n",
      "Loss after 336192 examples: 0.016\n",
      "Loss after 336392 examples: 0.025\n",
      "Loss after 336592 examples: 0.032\n",
      "Loss after 336792 examples: 0.029\n",
      "Loss after 336992 examples: 0.037\n",
      "Loss after 337192 examples: 0.026\n",
      "Loss after 337392 examples: 0.022\n",
      "Loss after 337592 examples: 0.021\n",
      "Loss after 337792 examples: 0.023\n",
      "Loss after 337992 examples: 0.023\n",
      "Loss after 338192 examples: 0.025\n",
      "Loss after 338392 examples: 0.031\n",
      "Loss after 338592 examples: 0.027\n",
      "Loss after 338792 examples: 0.020\n",
      "Loss after 338992 examples: 0.028\n",
      "Loss after 339192 examples: 0.033\n",
      "Loss after 339392 examples: 0.040\n",
      "Loss after 339592 examples: 0.045\n",
      "Loss after 339792 examples: 0.076\n",
      "Loss after 339992 examples: 0.031\n",
      "Loss after 340192 examples: 0.041\n",
      "Loss after 340392 examples: 0.024\n",
      "Loss after 340592 examples: 0.029\n",
      "Loss after 340792 examples: 0.035\n",
      "Loss after 340992 examples: 0.029\n",
      "Loss after 341192 examples: 0.031\n",
      "Loss after 341392 examples: 0.038\n",
      "Loss after 341592 examples: 0.039\n",
      "Loss after 341792 examples: 0.027\n",
      "Loss after 341992 examples: 0.033\n",
      "Loss after 342192 examples: 0.036\n",
      "Loss after 342392 examples: 0.023\n",
      "Loss after 342592 examples: 0.026\n",
      "Loss after 342792 examples: 0.020\n",
      "Loss after 342992 examples: 0.019\n",
      "Loss after 343192 examples: 0.033\n",
      "Loss after 343392 examples: 0.020\n",
      "Loss after 343592 examples: 0.024\n",
      "Loss after 343792 examples: 0.033\n",
      "Loss after 343992 examples: 0.022\n",
      "Loss after 344192 examples: 0.027\n",
      "Loss after 344392 examples: 0.031\n",
      "Loss after 344592 examples: 0.029\n",
      "Loss after 344792 examples: 0.030\n",
      "Loss after 344992 examples: 0.040\n",
      "Loss after 345192 examples: 0.034\n",
      "Loss after 345392 examples: 0.033\n",
      "Loss after 345592 examples: 0.039\n",
      "Loss after 345792 examples: 0.035\n",
      "Loss after 345992 examples: 0.020\n",
      "Loss after 346192 examples: 0.016\n",
      "Loss after 346392 examples: 0.029\n",
      "Loss after 346592 examples: 0.022\n",
      "Loss after 346792 examples: 0.029\n",
      "Loss after 346992 examples: 0.022\n",
      "Loss after 347192 examples: 0.048\n",
      "Loss after 347392 examples: 0.034\n",
      "Loss after 347592 examples: 0.027\n",
      "Loss after 347792 examples: 0.025\n",
      "Loss after 347992 examples: 0.025\n",
      "Loss after 348192 examples: 0.032\n",
      "Loss after 348392 examples: 0.026\n",
      "Loss after 348592 examples: 0.021\n",
      "Loss after 348792 examples: 0.037\n",
      "Loss after 348992 examples: 0.021\n",
      "Loss after 349192 examples: 0.028\n",
      "Loss after 349392 examples: 0.024\n",
      "Loss after 349592 examples: 0.036\n",
      "Loss after 349792 examples: 0.054\n",
      "Loss after 349992 examples: 0.029\n",
      "Train Loss: 0.014\n",
      "Test Loss: 0.018\n",
      "[0.0001]\n",
      "Loss after 350192 examples: 0.026\n",
      "Loss after 350392 examples: 0.021\n",
      "Loss after 350592 examples: 0.014\n",
      "Loss after 350792 examples: 0.016\n",
      "Loss after 350992 examples: 0.028\n",
      "Loss after 351192 examples: 0.014\n",
      "Loss after 351392 examples: 0.018\n",
      "Loss after 351592 examples: 0.023\n",
      "Loss after 351792 examples: 0.016\n",
      "Loss after 351992 examples: 0.038\n",
      "Loss after 352192 examples: 0.025\n",
      "Loss after 352392 examples: 0.027\n",
      "Loss after 352592 examples: 0.026\n",
      "Loss after 352792 examples: 0.021\n",
      "Loss after 352992 examples: 0.019\n",
      "Loss after 353192 examples: 0.032\n",
      "Loss after 353392 examples: 0.023\n",
      "Loss after 353592 examples: 0.014\n",
      "Loss after 353792 examples: 0.041\n",
      "Loss after 353992 examples: 0.032\n",
      "Loss after 354192 examples: 0.034\n",
      "Loss after 354392 examples: 0.027\n",
      "Loss after 354592 examples: 0.030\n",
      "Loss after 354792 examples: 0.021\n",
      "Loss after 354992 examples: 0.028\n",
      "Loss after 355192 examples: 0.034\n",
      "Loss after 355392 examples: 0.015\n",
      "Loss after 355592 examples: 0.047\n",
      "Loss after 355792 examples: 0.032\n",
      "Loss after 355992 examples: 0.020\n",
      "Loss after 356192 examples: 0.029\n",
      "Loss after 356392 examples: 0.037\n",
      "Loss after 356592 examples: 0.024\n",
      "Loss after 356792 examples: 0.033\n",
      "Loss after 356992 examples: 0.033\n",
      "Loss after 357192 examples: 0.041\n",
      "Loss after 357392 examples: 0.034\n",
      "Loss after 357592 examples: 0.015\n",
      "Loss after 357792 examples: 0.045\n",
      "Loss after 357992 examples: 0.020\n",
      "Loss after 358192 examples: 0.019\n",
      "Loss after 358392 examples: 0.019\n",
      "Loss after 358592 examples: 0.009\n",
      "Loss after 358792 examples: 0.014\n",
      "Loss after 358992 examples: 0.031\n",
      "Loss after 359192 examples: 0.019\n",
      "Loss after 359392 examples: 0.025\n",
      "Loss after 359592 examples: 0.018\n",
      "Loss after 359792 examples: 0.013\n",
      "Loss after 359992 examples: 0.024\n",
      "Loss after 360192 examples: 0.023\n",
      "Loss after 360392 examples: 0.029\n",
      "Loss after 360592 examples: 0.017\n",
      "Loss after 360792 examples: 0.019\n",
      "Loss after 360992 examples: 0.027\n",
      "Loss after 361192 examples: 0.034\n",
      "Loss after 361392 examples: 0.013\n",
      "Loss after 361592 examples: 0.022\n",
      "Loss after 361792 examples: 0.014\n",
      "Loss after 361992 examples: 0.015\n",
      "Loss after 362192 examples: 0.040\n",
      "Loss after 362392 examples: 0.020\n",
      "Loss after 362592 examples: 0.021\n",
      "Loss after 362792 examples: 0.016\n",
      "Loss after 362992 examples: 0.031\n",
      "Loss after 363192 examples: 0.026\n",
      "Loss after 363392 examples: 0.023\n",
      "Loss after 363592 examples: 0.025\n",
      "Loss after 363792 examples: 0.014\n",
      "Loss after 363992 examples: 0.015\n",
      "Loss after 364192 examples: 0.021\n",
      "Loss after 364392 examples: 0.039\n",
      "Loss after 364592 examples: 0.019\n",
      "Loss after 364792 examples: 0.031\n",
      "Loss after 364992 examples: 0.027\n",
      "Loss after 365192 examples: 0.026\n",
      "Loss after 365392 examples: 0.017\n",
      "Loss after 365592 examples: 0.021\n",
      "Loss after 365792 examples: 0.049\n",
      "Loss after 365992 examples: 0.015\n",
      "Loss after 366192 examples: 0.030\n",
      "Loss after 366392 examples: 0.032\n",
      "Loss after 366592 examples: 0.017\n",
      "Loss after 366792 examples: 0.030\n",
      "Loss after 366992 examples: 0.025\n",
      "Loss after 367192 examples: 0.022\n",
      "Loss after 367392 examples: 0.022\n",
      "Loss after 367592 examples: 0.018\n",
      "Loss after 367792 examples: 0.021\n",
      "Loss after 367992 examples: 0.028\n",
      "Loss after 368192 examples: 0.035\n",
      "Loss after 368392 examples: 0.029\n",
      "Loss after 368592 examples: 0.021\n",
      "Loss after 368792 examples: 0.015\n",
      "Loss after 368992 examples: 0.029\n",
      "Loss after 369192 examples: 0.014\n",
      "Loss after 369392 examples: 0.022\n",
      "Loss after 369592 examples: 0.020\n",
      "Loss after 369792 examples: 0.037\n",
      "Loss after 369992 examples: 0.030\n",
      "Loss after 370192 examples: 0.019\n",
      "Loss after 370392 examples: 0.019\n",
      "Loss after 370592 examples: 0.014\n",
      "Loss after 370792 examples: 0.050\n",
      "Loss after 370992 examples: 0.033\n",
      "Loss after 371192 examples: 0.037\n",
      "Loss after 371392 examples: 0.021\n",
      "Loss after 371592 examples: 0.009\n",
      "Loss after 371792 examples: 0.030\n",
      "Loss after 371992 examples: 0.030\n",
      "Loss after 372192 examples: 0.020\n",
      "Loss after 372392 examples: 0.028\n",
      "Loss after 372592 examples: 0.020\n",
      "Loss after 372792 examples: 0.027\n",
      "Loss after 372992 examples: 0.026\n",
      "Loss after 373192 examples: 0.019\n",
      "Loss after 373392 examples: 0.020\n",
      "Loss after 373592 examples: 0.037\n",
      "Loss after 373792 examples: 0.020\n",
      "Loss after 373992 examples: 0.041\n",
      "Loss after 374192 examples: 0.034\n",
      "Loss after 374392 examples: 0.027\n",
      "Loss after 374592 examples: 0.030\n",
      "Loss after 374792 examples: 0.019\n",
      "Loss after 374992 examples: 0.022\n",
      "Loss after 375192 examples: 0.029\n",
      "Loss after 375392 examples: 0.021\n",
      "Loss after 375592 examples: 0.040\n",
      "Loss after 375792 examples: 0.020\n",
      "Loss after 375992 examples: 0.022\n",
      "Loss after 376192 examples: 0.020\n",
      "Loss after 376392 examples: 0.023\n",
      "Loss after 376592 examples: 0.011\n",
      "Loss after 376792 examples: 0.028\n",
      "Loss after 376992 examples: 0.028\n",
      "Loss after 377192 examples: 0.029\n",
      "Loss after 377392 examples: 0.009\n",
      "Loss after 377592 examples: 0.025\n",
      "Loss after 377792 examples: 0.017\n",
      "Loss after 377992 examples: 0.022\n",
      "Loss after 378192 examples: 0.031\n",
      "Loss after 378392 examples: 0.034\n",
      "Loss after 378592 examples: 0.029\n",
      "Loss after 378792 examples: 0.020\n",
      "Loss after 378992 examples: 0.015\n",
      "Loss after 379192 examples: 0.035\n",
      "Loss after 379392 examples: 0.022\n",
      "Loss after 379592 examples: 0.027\n",
      "Loss after 379792 examples: 0.023\n",
      "Loss after 379992 examples: 0.024\n",
      "Loss after 380192 examples: 0.038\n",
      "Loss after 380392 examples: 0.018\n",
      "Loss after 380592 examples: 0.034\n",
      "Loss after 380792 examples: 0.015\n",
      "Loss after 380992 examples: 0.019\n",
      "Loss after 381192 examples: 0.022\n",
      "Loss after 381392 examples: 0.033\n",
      "Loss after 381592 examples: 0.026\n",
      "Loss after 381792 examples: 0.022\n",
      "Loss after 381992 examples: 0.038\n",
      "Loss after 382192 examples: 0.023\n",
      "Loss after 382392 examples: 0.014\n",
      "Loss after 382592 examples: 0.018\n",
      "Loss after 382792 examples: 0.015\n",
      "Loss after 382992 examples: 0.020\n",
      "Loss after 383192 examples: 0.034\n",
      "Loss after 383392 examples: 0.037\n",
      "Loss after 383592 examples: 0.038\n",
      "Loss after 383792 examples: 0.036\n",
      "Loss after 383992 examples: 0.040\n",
      "Loss after 384192 examples: 0.026\n",
      "Loss after 384392 examples: 0.031\n",
      "Loss after 384592 examples: 0.023\n",
      "Loss after 384792 examples: 0.032\n",
      "Loss after 384992 examples: 0.031\n",
      "Loss after 385192 examples: 0.026\n",
      "Loss after 385392 examples: 0.026\n",
      "Loss after 385592 examples: 0.048\n",
      "Loss after 385792 examples: 0.032\n",
      "Loss after 385992 examples: 0.020\n",
      "Loss after 386192 examples: 0.020\n",
      "Loss after 386392 examples: 0.027\n",
      "Loss after 386592 examples: 0.028\n",
      "Loss after 386792 examples: 0.019\n",
      "Loss after 386992 examples: 0.026\n",
      "Loss after 387192 examples: 0.026\n",
      "Loss after 387392 examples: 0.022\n",
      "Loss after 387592 examples: 0.032\n",
      "Loss after 387792 examples: 0.051\n",
      "Loss after 387992 examples: 0.028\n",
      "Loss after 388192 examples: 0.012\n",
      "Loss after 388392 examples: 0.023\n",
      "Loss after 388592 examples: 0.020\n",
      "Loss after 388792 examples: 0.019\n",
      "Loss after 388992 examples: 0.020\n",
      "Loss after 389192 examples: 0.023\n",
      "Loss after 389392 examples: 0.022\n",
      "Loss after 389592 examples: 0.029\n",
      "Loss after 389792 examples: 0.021\n",
      "Loss after 389992 examples: 0.024\n",
      "Loss after 390192 examples: 0.022\n",
      "Loss after 390392 examples: 0.023\n",
      "Loss after 390592 examples: 0.034\n",
      "Loss after 390792 examples: 0.022\n",
      "Loss after 390992 examples: 0.028\n",
      "Loss after 391192 examples: 0.020\n",
      "Loss after 391392 examples: 0.038\n",
      "Loss after 391592 examples: 0.030\n",
      "Loss after 391792 examples: 0.033\n",
      "Loss after 391992 examples: 0.018\n",
      "Loss after 392192 examples: 0.024\n",
      "Loss after 392392 examples: 0.026\n",
      "Loss after 392592 examples: 0.034\n",
      "Loss after 392792 examples: 0.022\n",
      "Loss after 392992 examples: 0.034\n",
      "Loss after 393192 examples: 0.019\n",
      "Loss after 393392 examples: 0.020\n",
      "Loss after 393592 examples: 0.025\n",
      "Loss after 393792 examples: 0.013\n",
      "Loss after 393992 examples: 0.019\n",
      "Loss after 394192 examples: 0.027\n",
      "Loss after 394392 examples: 0.028\n",
      "Loss after 394592 examples: 0.018\n",
      "Loss after 394792 examples: 0.028\n",
      "Loss after 394992 examples: 0.033\n",
      "Loss after 395192 examples: 0.039\n",
      "Loss after 395392 examples: 0.020\n",
      "Loss after 395592 examples: 0.026\n",
      "Loss after 395792 examples: 0.021\n",
      "Loss after 395992 examples: 0.023\n",
      "Loss after 396192 examples: 0.020\n",
      "Loss after 396392 examples: 0.029\n",
      "Loss after 396592 examples: 0.039\n",
      "Loss after 396792 examples: 0.019\n",
      "Loss after 396992 examples: 0.034\n",
      "Loss after 397192 examples: 0.020\n",
      "Loss after 397392 examples: 0.028\n",
      "Loss after 397592 examples: 0.026\n",
      "Loss after 397792 examples: 0.017\n",
      "Loss after 397992 examples: 0.040\n",
      "Loss after 398192 examples: 0.026\n",
      "Loss after 398392 examples: 0.021\n",
      "Loss after 398592 examples: 0.020\n",
      "Loss after 398792 examples: 0.037\n",
      "Loss after 398992 examples: 0.012\n",
      "Loss after 399192 examples: 0.014\n",
      "Loss after 399392 examples: 0.029\n",
      "Loss after 399592 examples: 0.021\n",
      "Loss after 399792 examples: 0.027\n",
      "Loss after 399992 examples: 0.028\n",
      "Train Loss: 0.011\n",
      "Test Loss: 0.016\n",
      "[0.0001]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb435d0fc7354c149cfc1d0e25b1d5d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.297 MB of 0.297 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td></td></tr><tr><td>Test accuracy</td><td></td></tr><tr><td>Test edit</td><td></td></tr><tr><td>Test loss</td><td></td></tr><tr><td>Train accuracy</td><td></td></tr><tr><td>Train edit</td><td></td></tr><tr><td>Train loss</td><td></td></tr><tr><td>loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>7</td></tr><tr><td>Test accuracy</td><td>0.9978</td></tr><tr><td>Test edit</td><td>0.0028</td></tr><tr><td>Test loss</td><td>0.01622</td></tr><tr><td>Train accuracy</td><td>0.99834</td></tr><tr><td>Train edit</td><td>0.00166</td></tr><tr><td>Train loss</td><td>0.01148</td></tr><tr><td>loss</td><td>0.02797</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lemon-cloud-10</strong> at: <a href='https://wandb.ai/marino-com/phoc-def/runs/2y7o6nyc' target=\"_blank\">https://wandb.ai/marino-com/phoc-def/runs/2y7o6nyc</a><br/> View project at: <a href='https://wandb.ai/marino-com/phoc-def' target=\"_blank\">https://wandb.ai/marino-com/phoc-def</a><br/>Synced 6 W&B file(s), 80 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240530_003443-2y7o6nyc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ensure GPU usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Caller function to train the model\n",
    "def model_pipeline(cfg:dict) -> None:\n",
    "    with wandb.init(project=\"phoc-def\", config=cfg): # Set up of project\n",
    "        config = wandb.config\n",
    "\n",
    "        model, train_loader, test_loader, criterion, optimizer, scheduler = make(config, device)\n",
    "        model = train(model, train_loader, test_loader, criterion, optimizer, scheduler, config, device)\n",
    "\n",
    "        return model\n",
    "\n",
    "# Actual main, with wandb implementation\n",
    "if __name__ == \"__main__\":\n",
    "    wandb.login()\n",
    "\n",
    "    config = dict(\n",
    "        train_dir='ADD PATH'+\"/\",\n",
    "        test_dir='ADD PATH'+\"/\",\n",
    "        epochs=8,\n",
    "        batch_size= 8,\n",
    "        learning_rate=0.0001,\n",
    "        save_model = saved_model_phocnet+\"/\")\n",
    "    model = model_pipeline(config)      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py38_PT_and_TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
