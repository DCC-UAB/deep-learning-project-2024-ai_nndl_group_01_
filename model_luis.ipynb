{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_synthetic = 'Synthetic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\anaconda3\\envs\\DeepLearning\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] No se encontró el proceso especificado'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Function to extract image paths and labels with corrected paths, starting from the first files\n",
    "def extract_image_paths_and_labels_corrected(annotation_file, base_folder, num_images):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    with open(annotation_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    lines.sort()  # Sort the lines to ensure we start from the first files\n",
    "    \n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) < 2:\n",
    "            continue\n",
    "        image_path, _ = parts\n",
    "        label = image_path.split('_')[1]\n",
    "        # Correct the path separators\n",
    "        corrected_image_path = os.path.join(base_folder, image_path[2:]).replace('\\\\', '/')\n",
    "        image_paths.append(corrected_image_path)\n",
    "        labels.append(label)\n",
    "        if len(image_paths) >= num_images:\n",
    "            break\n",
    "    return image_paths, labels\n",
    "\n",
    "\n",
    "# Paths to annotation files and base folders\n",
    "training_folder = 'Synthetic/training_folder'\n",
    "validation_folder = 'Synthetic/validation_folder'\n",
    "test_folder = 'Synthetic/test_folder'\n",
    "\n",
    "annotation_train_file = 'Synthetic/annotation_train.txt'\n",
    "annotation_val_file = 'Synthetic/annotation_val.txt'\n",
    "annotation_test_file = 'Synthetic/annotation_test.txt'\n",
    "\n",
    "# Extract data with corrected image paths\n",
    "train_images, train_labels = extract_image_paths_and_labels_corrected(annotation_train_file, training_folder, 200000)\n",
    "val_images, val_labels = extract_image_paths_and_labels_corrected(annotation_val_file, validation_folder, 10000)\n",
    "test_images, test_labels = extract_image_paths_and_labels_corrected(annotation_test_file, test_folder, 10000)\n",
    "\n",
    "# Create dataframes\n",
    "train_df = pd.DataFrame({'image_path': train_images, 'label': train_labels})\n",
    "val_df = pd.DataFrame({'image_path': val_images, 'label': val_labels})\n",
    "test_df = pd.DataFrame({'image_path': test_images, 'label': test_labels})\n",
    "\n",
    "# Save to CSV files\n",
    "train_df.to_csv('train_data.csv', index=False)\n",
    "val_df.to_csv('val_data.csv', index=False)\n",
    "test_df.to_csv('test_data.csv', index=False)\n",
    "\n",
    "# Create dictionaries from the dataframes\n",
    "train_dict = train_df.to_dict(orient='records')\n",
    "val_dict = val_df.to_dict(orient='records')\n",
    "test_dict = test_df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train_data.csv\")\n",
    "test_data = pd.read_csv(\"test_data.csv\")\n",
    "val_data = pd.read_csv(\"val_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "l =  train_data['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hay\n"
     ]
    }
   ],
   "source": [
    "if 'GUNS' in l:\n",
    "    print(\"hay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31, 81, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "img = cv2.imread('Synthetic/training_folder/1/1/100_Classmates_13991.jpg')\n",
    "\n",
    "cv2.imshow('image', img)\n",
    "plt.imshow(img)\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def build_phoc(words, phoc_unigrams, unigram_levels, phoc_bigrams=None, bigram_levels=None, split_character=None):\n",
    "    '''\n",
    "    Calculate Pyramidal Histogram of Characters (PHOC) descriptor (see Almazan 2014).\n",
    "    Args:\n",
    "        words (list of str): list of words to calculate descriptor for\n",
    "        phoc_unigrams (str): string of all unigrams to use in the PHOC\n",
    "        unigram_levels (list of int): the levels for the unigrams in PHOC\n",
    "        phoc_bigrams (list of str): list of bigrams to be used in the PHOC\n",
    "        bigram_levels (list of int): the levels of the bigrams in the PHOC\n",
    "        split_character (str): special character to split the word strings into characters (default is None)\n",
    "    Returns:\n",
    "        the PHOC for the given words as a numpy array\n",
    "    '''\n",
    "    if split_character is None:\n",
    "        split_character = ''\n",
    "    \n",
    "    # normalized occupancy interval\n",
    "    def occupancy(k, n):\n",
    "        return (k / n, (k + 1) / n)\n",
    "\n",
    "    # overlap between two intervals\n",
    "    def overlap(a, b):\n",
    "        return max(0, min(a[1], b[1]) - max(a[0], b[0]))\n",
    "\n",
    "    # Initialize PHOC descriptors  levels x len(phoc) (2+3+4+5) X 36 + 2 X 50 = 504 + 100 = 604\n",
    "    phoc_size = sum(unigram_levels) * len(phoc_unigrams)\n",
    "    if phoc_bigrams:\n",
    "        phoc_size += sum(bigram_levels) * len(phoc_bigrams)\n",
    "    phocs = np.zeros((len(words), phoc_size))\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        word_len = len(word)\n",
    "        \n",
    "        # Calculate unigram features\n",
    "        index = 0\n",
    "        for level in unigram_levels:\n",
    "            region_len = word_len / level\n",
    "            for region in range(level):\n",
    "                region_start = region / level\n",
    "                region_end = (region + 1) / level\n",
    "                region_occ = (region_start, region_end)\n",
    "                for char in phoc_unigrams:\n",
    "                    char_occ = [occupancy(k, word_len) for k, c in enumerate(word) if c == char]\n",
    "                    if any(overlap(region_occ, occ) >= 0.5 * (occ[1] - occ[0]) for occ in char_occ):\n",
    "                        phocs[i, index] = 1\n",
    "                    index += 1\n",
    "        \n",
    "        # Calculate bigram features\n",
    "        if phoc_bigrams:\n",
    "            for level in bigram_levels:\n",
    "                region_len = word_len / level\n",
    "                for region in range(level):\n",
    "                    region_start = region / level\n",
    "                    region_end = (region + 1) / level\n",
    "                    region_occ = (region_start, region_end)\n",
    "                    for bigram in phoc_bigrams:\n",
    "                        bigram_occ = [occupancy(k, word_len) for k in range(word_len - 1) if word[k:k+2] == bigram]\n",
    "                        if any(overlap(region_occ, occ) >= 0.5 * (occ[1] - occ[0]) for occ in bigram_occ):\n",
    "                            phocs[i, index] = 1\n",
    "                        index += 1\n",
    "\n",
    "    return phocs\n",
    "\n",
    "def phoc(raw_word):\n",
    "    '''\n",
    "    Convert a word into its PHOC representation.\n",
    "    \n",
    "    :param raw_word: string of word to be converted\n",
    "    :return: phoc representation as a np.array (1, 604)\n",
    "    '''\n",
    "    phoc_unigrams = 'abcdefghijklmnopqrstuvwxyz0123456789'\n",
    "    unigram_levels = [2, 3, 4, 5]\n",
    "    phoc_bigrams = ['th', 'he', 'in', 'er', 'an',\n",
    "                     're', 'on', 'at', 'en', 'nd',\n",
    "                      'ti', 'es', 'or', 'te', 'of',\n",
    "                       'ed', 'is', 'it', 'al', 'ar',\n",
    "                        'st', 'to', 'nt', 'ng', 'se',\n",
    "                         'ha', 'as', 'ou', 'io', 'le',\n",
    "                          've', 'co', 'me', 'de', 'hi',\n",
    "                           'ri', 'ro', 'ic', 'ne', 'ea',\n",
    "                            'ra', 'ce', 'li', 'ch', 'll',\n",
    "                             'be', 'ma', 'si', 'om', 'ur']\n",
    "    bigram_levels = [2]\n",
    "\n",
    "    return build_phoc([raw_word], phoc_unigrams, unigram_levels, phoc_bigrams, bigram_levels)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['th', 'he', 'in', 'er', 'an', 're', 'on', 'at', 'en', 'nd', 'ti', 'es', 'or', 'te', 'of', 'ed', 'is', 'it', 'al', 'ar', 'st', 'to', 'nt', 'ng', 'se', 'ha', 'as', 'ou', 'io', 'le', 've', 'co', 'me', 'de', 'hi', 'ri', 'ro', 'ic', 'ne', 'ea', 'ra', 'ce', 'li', 'ch', 'll', 'be', 'ma', 'si', 'om', 'ur']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"def read_bigrams(file_txt):\n",
    "    res = []\n",
    "    with open(file_txt, 'r') as file:\n",
    "        for line in file:\n",
    "            bigram = line.split('\\n')\n",
    "            res.append(bigram[0])\n",
    "    return res\n",
    "print(read_bigrams('bigrams.txt'))\n",
    "l_bigrams = read_bigrams('bigrams.txt')\n",
    "\n",
    "phoc_bigrams = read_bigrams('bigrams.txt')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file_path, transform=None):\n",
    "        self.data_frame = pd.read_csv(csv_file_path)\n",
    "        self.transform = transform\n",
    "        self.label_to_index = {label: idx for idx, label in enumerate(self.data_frame['label'].unique())}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.data_frame.iloc[idx, 0]\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        str_label = self.data_frame.iloc[idx, 1].lower()\n",
    "        # label = self.label_to_index[label]¿?\n",
    "        phoc_label = phoc(str_label)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, str_label, phoc_label\n",
    "\n",
    "# Definir transformaciones\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Crear dataset y dataloader\n",
    "csv_file_path = 'train_data.csv'  # Reemplaza con la ruta a tu archivo CSV\n",
    "train_dataset = CustomDataset(csv_file_path, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "test_dataset = CustomDataset('val_data.csv',transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[-0.3541, -0.3541, -0.3541,  ..., -0.3883, -0.3883, -0.3883],\n",
      "         [-0.3541, -0.3541, -0.3541,  ..., -0.3883, -0.3883, -0.3883],\n",
      "         [-0.3541, -0.3541, -0.3541,  ..., -0.3883, -0.3883, -0.3883],\n",
      "         ...,\n",
      "         [-0.3883, -0.3883, -0.3712,  ..., -0.3369, -0.3369, -0.3369],\n",
      "         [-0.3883, -0.3883, -0.3712,  ..., -0.3369, -0.3369, -0.3369],\n",
      "         [-0.3883, -0.3883, -0.3712,  ..., -0.3369, -0.3369, -0.3369]],\n",
      "\n",
      "        [[-0.2325, -0.2325, -0.2325,  ..., -0.2675, -0.2675, -0.2675],\n",
      "         [-0.2325, -0.2325, -0.2325,  ..., -0.2675, -0.2675, -0.2675],\n",
      "         [-0.2325, -0.2325, -0.2325,  ..., -0.2675, -0.2675, -0.2675],\n",
      "         ...,\n",
      "         [-0.2675, -0.2675, -0.2500,  ..., -0.2150, -0.2150, -0.2150],\n",
      "         [-0.2675, -0.2675, -0.2500,  ..., -0.2150, -0.2150, -0.2150],\n",
      "         [-0.2675, -0.2675, -0.2500,  ..., -0.2150, -0.2150, -0.2150]],\n",
      "\n",
      "        [[-0.0092, -0.0092, -0.0092,  ..., -0.0441, -0.0441, -0.0441],\n",
      "         [-0.0092, -0.0092, -0.0092,  ..., -0.0441, -0.0441, -0.0441],\n",
      "         [-0.0092, -0.0092, -0.0092,  ..., -0.0441, -0.0441, -0.0441],\n",
      "         ...,\n",
      "         [-0.0441, -0.0441, -0.0267,  ...,  0.0082,  0.0082,  0.0082],\n",
      "         [-0.0441, -0.0441, -0.0267,  ...,  0.0082,  0.0082,  0.0082],\n",
      "         [-0.0441, -0.0441, -0.0267,  ...,  0.0082,  0.0082,  0.0082]]]), 'castling', array([1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 0., 0., 0., 0.]))\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SPP(nn.Module):\n",
    "\n",
    "    def __init__(self, levels = 3, pool_type = 'max_pool'):\n",
    "        super(SPP, self).__init__()\n",
    "\n",
    "        if pool_type not in ['max_pool', 'avg_pool', 'max_avg_pool']:\n",
    "            raise ValueError('Unknown pool_type. Must be either \\'max_pool\\', \\'avg_pool\\' or both')\n",
    "        \n",
    "        self.pooling_output_size = sum([4 ** level for level in range(levels)]) * 512\n",
    "        self.levels = levels\n",
    "        self.pool_type = pool_type\n",
    "\n",
    "    def forward(self, input_x):\n",
    "        out = self._spatial_pyramid_pooling(input_x, self.levels)\n",
    "        return out\n",
    "    \n",
    "    def _pyramid_pooling(self, input_x, output_sizes):\n",
    "        pyramid_level_tensors = []\n",
    "        for tsize in output_sizes:\n",
    "            if self.pool_type == 'max_pool':\n",
    "                pyramid_level_tensor = F.adaptive_max_pool2d(input_x, tsize)\n",
    "                pyramid_level_tensor = pyramid_level_tensor.view(input_x.size(0), -1)\n",
    "            if self.pool_type == 'avg_pool':\n",
    "                pyramid_level_tensor = F.adaptive_avg_pool2d(input_x, tsize)\n",
    "                pyramid_level_tensor = pyramid_level_tensor.view(input_x.size(0), -1)\n",
    "            if self.pool_type == 'max_avg_pool':\n",
    "                pyramid_level_tensor_max = F.adaptive_max_pool2d(input_x, tsize)\n",
    "                pyramid_level_tensor_max = pyramid_level_tensor_max.view(input_x.size(0), -1)\n",
    "                pyramid_level_tensor_avg = F.adaptive_avg_pool2d(input_x, tsize)\n",
    "                pyramid_level_tensor_avg = pyramid_level_tensor_avg.view(input_x.size(0), -1)\n",
    "                pyramid_level_tensor = torch.cat([pyramid_level_tensor_max, pyramid_level_tensor_avg], dim=1)\n",
    "\n",
    "            pyramid_level_tensors.append(pyramid_level_tensor)\n",
    "\n",
    "        return torch.cat(pyramid_level_tensors, dim=1)\n",
    "\n",
    "    def _spatial_pyramid_pooling(self, input_x, levels):\n",
    "        output_sizes = [(int( 2 **level), int( 2 **level)) for level in range(levels)]\n",
    "        return self._pyramid_pooling(input_x, output_sizes)\n",
    "\n",
    "# Phocnet implementation from PHOCNet GH Repo\n",
    "class PHOCNet(nn.Module):\n",
    "\n",
    "    def __init__(self, n_out, input_channels = 3, pooling_levels = 3, pool_type = 'max_pool'):\n",
    "        super(PHOCNet, self).__init__()\n",
    "\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_channels, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv_block4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.pooling_layer_fn = SPP(levels = pooling_levels, pool_type=pool_type)\n",
    "        pooling_output_size = self.pooling_layer_fn.pooling_output_size\n",
    "        \n",
    "        self.fc1 = nn.Linear(pooling_output_size, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.Linear(4096, n_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_block1(x)\n",
    "        out = F.max_pool2d(out, kernel_size=2, stride=2, padding=0)\n",
    "        out = self.conv_block2(out)\n",
    "        out = F.max_pool2d(out, kernel_size=2, stride=2, padding=0)\n",
    "        out = self.conv_block3(out)\n",
    "        out = self.conv_block4(out)\n",
    "\n",
    "        out = self.pooling_layer_fn(out)\n",
    "        out = self.fc1(out)\n",
    "        out = F.relu(out)\n",
    "        out = F.dropout(out, p = 0.5, training = self.training)\n",
    "        out = self.fc2(out)\n",
    "        out = F.relu(out)\n",
    "        out = F.dropout(out, p = 0.5, training = self.training)\n",
    "        out = self.fc3(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    # distribución normal con media cero\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias.data, 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, test_loader, criterion, optimizer, scheduler, config, device = \"cuda\"):\n",
    "    # Tell wandb to watch what the model gets up to: gradients, weights, and more!\n",
    "    wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
    "\n",
    "    example_ct = 0  # number of examples seen\n",
    "    batch_ct = 0\n",
    "    model_phoc = load_model()\n",
    "    for epoch in tqdm(range(config.epochs)):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for _, (images, phoc_labels, _) in enumerate(train_loader):\n",
    "\n",
    "            loss = train_batch(images, phoc_labels, model, optimizer, criterion, device)\n",
    "            train_loss += loss.item()\n",
    "            example_ct +=  len(images)\n",
    "            batch_ct += 1\n",
    "\n",
    "            #Report metrics every 25th batch\n",
    "            if ((batch_ct + 1) % 25) == 0:\n",
    "                train_log(loss.item(), example_ct)\n",
    "        \n",
    "        # loss_test = test(model, test_loader, train_loader, epoch, criterion, model_phoc, device)\n",
    "        \n",
    "        # scheduler.step(loss_test)\n",
    "        # print(scheduler._last_lr)\n",
    "        torch.save(model.state_dict(), os.path.join(config.save_model, f\"PHOCNET{epoch}.pt\"))\n",
    "    return model\n",
    "\n",
    "def train_batch(images, labels, model, optimizer, criterion, device=\"cuda\"):\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    \n",
    "    # Forward pass ➡\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs.float(), labels.float())\n",
    "    \n",
    "    # Backward pass ⬅\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Step with optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test(model, test_loader, train_loader, epoch, criterion, model_phoc, device=\"cuda\", save:bool= True):\n",
    "    # Run the model on some test examples\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss_test = 0\n",
    "        loss_train = 0\n",
    "        correct_test = 0\n",
    "        correct_train = 0\n",
    "        edit_test = 0\n",
    "        edit_train = 0\n",
    "        test_count = 0\n",
    "        train_count = 0\n",
    "        \n",
    "        for i, (images, phoc_labels, text_labels) in enumerate(test_loader):\n",
    "            images, phoc_labels = images.to(device), phoc_labels.to(device)\n",
    "            test_count += len(images)\n",
    "            outputs = model(images)\n",
    "            loss_test += criterion(outputs, phoc_labels.float()) \n",
    "            predicted_labels = predict_with_PHOC(torch.sigmoid(outputs).cpu().numpy(), model_phoc)\n",
    "            correct_test += (predicted_labels == text_labels).sum().item()\n",
    "            edit_test += sum([editdistance.eval(p,t) for p,t in zip(predicted_labels, text_labels)])\n",
    "            if i == 0:\n",
    "                log_images(images, predicted_labels, text_labels[:5], epoch, \"Test\")\n",
    "                \n",
    "        for i, (images, phoc_labels, text_labels) in enumerate(train_loader):\n",
    "            images, phoc_labels = images.to(device), phoc_labels.to(device)\n",
    "            train_count += len(images)\n",
    "            outputs = model(images)\n",
    "            loss_train += criterion(outputs, phoc_labels.float()) \n",
    "            predicted_labels = predict_with_PHOC(torch.sigmoid(outputs).cpu().numpy(), model_phoc)\n",
    "            correct_train += (predicted_labels == text_labels).sum().item()\n",
    "            edit_train += sum([editdistance.eval(p,t) for p,t in zip(predicted_labels, text_labels)])\n",
    "            if i == 0:\n",
    "                log_images(images, predicted_labels, text_labels[:5], epoch, \"Train\")\n",
    "            if i == 150:\n",
    "               break\n",
    "\n",
    "        loss_test = loss_test/len(test_loader)\n",
    "        loss_train = loss_train/(i+1)    \n",
    "        accuracy_test = correct_test/test_count\n",
    "        accuracy_train = correct_train/train_count\n",
    "        edit_test = edit_test/test_count\n",
    "        edit_train = edit_train/train_count \n",
    "\n",
    "        train_test_log(loss_test, loss_train, accuracy_test, accuracy_train, edit_test, edit_train, epoch)\n",
    "    return loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_out = 604\n",
    "criterion = nn.CrossEntropylosws\n",
    "optimizer = \n",
    "scheduler = \n",
    "config = \n",
    "\n",
    "model = PHOCNet(n_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:dhwre718) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">resilient-music-16</strong> at: <a href='https://wandb.ai/uab-ai/Testing/runs/dhwre718' target=\"_blank\">https://wandb.ai/uab-ai/Testing/runs/dhwre718</a><br/> View project at: <a href='https://wandb.ai/uab-ai/Testing' target=\"_blank\">https://wandb.ai/uab-ai/Testing</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240522_143017-dhwre718\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:dhwre718). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Usuario\\Documents\\Universidad\\2º Año\\2º Semestre\\Neural Networks and Deep Learning\\Project\\wandb\\run-20240522_143116-5c63ee1n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/uab-ai/Testing/runs/5c63ee1n' target=\"_blank\">sparkling-dawn-17</a></strong> to <a href='https://wandb.ai/uab-ai/Testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/uab-ai/Testing' target=\"_blank\">https://wandb.ai/uab-ai/Testing</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/uab-ai/Testing/runs/5c63ee1n' target=\"_blank\">https://wandb.ai/uab-ai/Testing/runs/5c63ee1n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(model, train_loader, test_loader, criterion, optimizer, scheduler, config, device = \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"phocnet_project\", config={\"epochs\": 20, \"batch_size\": 32, \"learning_rate\": 0.001})\n",
    "config = wandb.config\n",
    "\n",
    "model = PHOCNet(num_outputs=604).to('cuda')  # Ajusta el número de salidas según sea necesario\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "model = train(model, train_loader, test_loader, criterion, optimizer, scheduler, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
